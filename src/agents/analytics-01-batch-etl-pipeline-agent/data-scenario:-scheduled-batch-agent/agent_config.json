{
  "bundle_id": "14e0fce4-5732-4ef2-9915-97f6404772e0",
  "name": "data-scenario:-scheduled-batch-agent",
  "version": "1.0.0",
  "metadata": {
    "domain": "data-processing",
    "complexity": "complex",
    "phase_count": 4,
    "skill_count": 2,
    "estimated_duration": "2 hours 36 minutes",
    "required_capabilities": [
      "validation",
      "data-ingestion",
      "parsing",
      "pattern-detection",
      "visualization",
      "transformation",
      "reporting",
      "analysis"
    ],
    "skill_names": [
      "documenter",
      "data-processor"
    ],
    "parallel_opportunities": 0,
    "risk_factors": [
      "High complexity may require extended execution time",
      "Large data volumes may cause performance issues"
    ]
  },
  "auto_mode_config": {
    "max_turns": 18,
    "initial_prompt": "# Goal: Scenario: Scheduled Batch ETL Pipeline\n\n## Objective\n# Scenario: Scheduled Batch ETL Pipeline\n\n## Technology Area\nAnalytics\n\n## Company Profile\n- **Company Size**: Mid-size retail company\n- **Industry**: Retail / E-commerce\n- **Use Case**: Daily sales data processing from multiple stores into a central data warehouse for business intelligence reporting\n\n## Scenario Description\nExtract daily sales data from cloud storage (CSV files uploaded by each store), transform the data to clean and aggregate it, and load it into Azure SQL Database for Power BI dashboards. The pipeline runs automatically on a daily schedule.\n\n## Azure Services Used\n- Azure Storage Account (Data Lake Gen2)\n- Azure Data Factory\n- Azure SQL Database\n- Azure Key Vault (for connection strings)\n\n## Prerequisites\n- Azure subscription with Contributor role\n- Azure CLI installed (`az --version`)\n- A unique identifier for this scenario run (e.g., timestamp or GUID)\n\n---\n\n## Phase 1: Deployment and Validation\n\n### Environment Setup\n```bash\n# Set variables\nUNIQUE_ID=$(date +%Y%m%d%H%M%S)\nRESOURCE_GROUP=\"azurehaymaker-analytics-etl-${UNIQUE_ID}-rg\"\nLOCATION=\"eastus\"\nSTORAGE_ACCOUNT=\"azmkretl${UNIQUE_ID}\"\nSQL_SERVER=\"azurehaymaker-sql-${UNIQUE_ID}\"\nSQL_DB=\"saleswarehouse\"\nDATA_FACTORY=\"azurehaymaker-adf-${UNIQUE_ID}\"\nKEYVAULT=\"azurehaymaker-kv-${UNIQUE_ID}\"\nSQL_ADMIN_USER=\"sqladmin\"\nSQL_ADMIN_PASSWORD=\"P@ssw0rd!${UNIQUE_ID}\"\n\n# Tags\nTAGS=\"AzureHayMaker-managed=true Scenario=analytics-batch-etl Owner=AzureHayMaker\"\n```\n\n### Deployment Steps\n```bash\n# Step 1: Create Resource Group\naz group create \\\n  --name \"${RESOURCE_GROUP}\" \\\n  --location \"${LOCATION}\" \\\n  --tags ${TAGS}\n\n# Step 2: Create Storage Account with Data Lake Gen2\naz storage account create \\\n  --name \"${STORAGE_ACCOUNT}\" \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --location \"${LOCATION}\" \\\n  --sku Standard_LRS \\\n  --kind StorageV2 \\\n  --hierarchical-namespace true \\\n  --tags ${TAGS}\n\n# Step 3: Create containers for raw and processed data\nSTORAGE_KEY=$(az storage account keys list --resource-group \"${RESOURCE_GROUP}\" --account-name \"${STORAGE_ACCOUNT}\" --query '[0].value' -o tsv)\n\naz storage container create \\\n  --name \"raw-sales-data\" \\\n  --account-name \"${STORAGE_ACCOUNT}\" \\\n  --account-key \"${STORAGE_KEY}\"\n\naz storage container create \\\n  --name \"processed-sales-data\" \\\n  --account-name \"${STORAGE_ACCOUNT}\" \\\n  --account-key \"${STORAGE_KEY}\"\n\n# Step 4: Upload sample data file\ncat > /tmp/sales_sample.csv <<EOF\nstore_id,date,product_id,quantity,revenue\n001,2024-01-15,P1001,5,125.50\n001,2024-01-15,P1002,3,89.99\n002,2024-01-15,P1001,8,200.00\nEOF\n\naz storage blob upload \\\n  --account-name \"${STORAGE_ACCOUNT}\" \\\n  --account-key \"${STORAGE_KEY}\" \\\n  --container-name \"raw-sales-data\" \\\n  --name \"sales_2024-01-15.csv\" \\\n  --file /tmp/sales_sample.csv\n\n# Step 5: Create Azure SQL Database\naz sql server create \\\n  --name \"${SQL_SERVER}\" \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --location \"${LOCATION}\" \\\n  --admin-user \"${SQL_ADMIN_USER}\" \\\n  --admin-password \"${SQL_ADMIN_PASSWORD}\" \\\n  --tags ${TAGS}\n\naz sql db create \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --server \"${SQL_SERVER}\" \\\n  --name \"${SQL_DB}\" \\\n  --service-objective S0 \\\n  --tags ${TAGS}\n\n# Step 6: Configure firewall to allow Azure services\naz sql server firewall-rule create \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --server \"${SQL_SERVER}\" \\\n  --name \"AllowAzureServices\" \\\n  --start-ip-address 0.0.0.0 \\\n  --end-ip-address 0.0.0.0\n\n# Step 7: Create target table in SQL Database\nSQL_CONNECTION_STRING=\"Server=tcp:${SQL_SERVER}.database.windows.net,1433;Database=${SQL_DB};User ID=${SQL_ADMIN_USER};Password=${SQL_ADMIN_PASSWORD};Encrypt=true;Connection Timeout=30;\"\n\nsqlcmd -S \"${SQL_SERVER}.database.windows.net\" -U \"${SQL_ADMIN_USER}\" -P \"${SQL_ADMIN_PASSWORD}\" -d \"${SQL_DB}\" -Q \"\nCREATE TABLE SalesFacts (\n    store_id VARCHAR(10),\n    sale_date DATE,\n    product_id VARCHAR(20),\n    quantity INT,\n    revenue DECIMAL(10,2),\n    load_timestamp DATETIME DEFAULT GETDATE()\n);\"\n\n# Step 8: Create Key Vault for secrets\naz keyvault create \\\n  --name \"${KEYVAULT}\" \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --location \"${LOCATION}\" \\\n  --tags ${TAGS}\n\naz keyvault secret set \\\n  --vault-name \"${KEYVAULT}\" \\\n  --name \"sql-connection-string\" \\\n  --value \"${SQL_CONNECTION_STRING}\"\n\naz keyvault secret set \\\n  --vault-name \"${KEYVAULT}\" \\\n  --name \"storage-account-key\" \\\n  --value \"${STORAGE_KEY}\"\n\n# Step 9: Create Azure Data Factory\naz datafactory create \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --factory-name \"${DATA_FACTORY}\" \\\n  --location \"${LOCATION}\" \\\n  --tags ${TAGS}\n\n# Step 10: Create Data Factory linked services and pipeline\n# Note: This requires JSON files for pipeline definition\n# For simplicity, we'll create a basic copy activity via Azure CLI\n\necho \"Data Factory created. Pipeline definition would go here.\"\necho \"In a full implementation, you would define:\"\necho \"  - Linked service for Storage Account\"\necho \"  - Linked service for SQL Database\"\necho \"  - Dataset for source CSV files\"\necho \"  - Dataset for destination SQL table\"\necho \"  - Pipeline with Copy Activity\"\necho \"  - Schedule trigger for daily execution\"\n```\n\n### Validation\n```bash\n# Verify Resource Group\naz group show --name \"${RESOURCE_GROUP}\"\n\n# Verify Storage Account\naz storage account show --name \"${STORAGE_ACCOUNT}\"\n\n# Verify SQL Server and Database\naz sql db show --resource-group \"${RESOURCE_GROUP}\" --server \"${SQL_SERVER}\" --name \"${SQL_DB}\"\n\n# Verify Data Factory\naz datafactory show --resource-group \"${RESOURCE_GROUP}\" --factory-name \"${DATA_FACTORY}\"\n\n# Verify Key Vault\naz keyvault show --name \"${KEYVAULT}\"\n\n# List all resources in resource group\naz resource list --resource-group \"${RESOURCE_GROUP}\" --output table\n```\n\n---\n\n## Phase 2: Mid-Day Operations and Management\n\n### Management Operations\n```bash\n# Operation 1: Upload new sales data file\nNEW_DATE=$(date +%Y-%m-%d)\ncat > /tmp/sales_${NEW_DATE}.csv <<EOF\nstore_id,date,product_id,quantity,revenue\n001,${NEW_DATE},P1001,12,300.00\n002,${NEW_DATE},P1003,7,175.99\nEOF\n\naz storage blob upload \\\n  --account-name \"${STORAGE_ACCOUNT}\" \\\n  --account-key \"${STORAGE_KEY}\" \\\n  --container-name \"raw-sales-data\" \\\n  --name \"sales_${NEW_DATE}.csv\" \\\n  --file /tmp/sales_${NEW_DATE}.csv \\\n  --overwrite\n\n# Operation 2: Manually trigger pipeline run (simulating scheduled execution)\necho \"In production, Data Factory pipeline would automatically trigger on schedule\"\necho \"Manual trigger command would be:\"\necho \"az datafactory pipeline create-run --resource-group \\\"${RESOURCE_GROUP}\\\" --factory-name \\\"${DATA_FACTORY}\\\" --name \\\"CopySalesToSQL\\\"\"\n\n# Operation 3: Check pipeline run status\necho \"Check pipeline runs:\"\necho \"az datafactory pipeline-run query-by-factory --resource-group \\\"${RESOURCE_GROUP}\\\" --factory-name \\\"${DATA_FACTORY}\\\"\"\n\n# Operation 4: Monitor storage account metrics\naz monitor metrics list \\\n  --resource \"/subscriptions/$(az account show --query id -o tsv)/resourceGroups/${RESOURCE_GROUP}/providers/Microsoft.Storage/storageAccounts/${STORAGE_ACCOUNT}\" \\\n  --metric \"Transactions\" \\\n  --start-time $(date -u -d '1 hour ago' '+%Y-%m-%dT%H:%M:%SZ') \\\n  --end-time $(date -u '+%Y-%m-%dT%H:%M:%SZ') \\\n  --interval PT1H\n\n# Operation 5: Monitor SQL Database DTU usage\naz sql db show \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --server \"${SQL_SERVER}\" \\\n  --name \"${SQL_DB}\" \\\n  --query \"currentServiceObjectiveName\"\n\n# Operation 6: Query data warehouse for record count\nsqlcmd -S \"${SQL_SERVER}.database.windows.net\" -U \"${SQL_ADMIN_USER}\" -P \"${SQL_ADMIN_PASSWORD}\" -d \"${SQL_DB}\" -Q \"SELECT COUNT(*) as RecordCount FROM SalesFacts;\"\n\n# Operation 7: List recent blob uploads\naz storage blob list \\\n  --account-name \"${STORAGE_ACCOUNT}\" \\\n  --account-key \"${STORAGE_KEY}\" \\\n  --container-name \"raw-sales-data\" \\\n  --output table\n```\n\n---\n\n## Phase 3: Cleanup and Tear-Down\n\n### Cleanup Steps\n```bash\n# Step 1: Delete the entire resource group (deletes all contained resources)\naz group delete \\\n  --name \"${RESOURCE_GROUP}\" \\\n  --yes \\\n  --no-wait\n\n# Step 2: Verify deletion (wait a few minutes for async deletion to complete)\nsleep 60\naz group exists --name \"${RESOURCE_GROUP}\"\n\n# Step 3: Verify no orphaned resources\necho \"Verifying cleanup...\"\naz resource list --resource-group \"${RESOURCE_GROUP}\" 2>&1 | grep \"could not be found\" && echo \"\u2713 Resource group successfully deleted\"\n```\n\n---\n\n## Resource Naming Convention\n- Resource Group: `azurehaymaker-analytics-etl-${UNIQUE_ID}-rg`\n- Storage Account: `azmkretl${UNIQUE_ID}`\n- SQL Server: `azurehaymaker-sql-${UNIQUE_ID}`\n- Data Factory: `azurehaymaker-adf-${UNIQUE_ID}`\n- Key Vault: `azurehaymaker-kv-${UNIQUE_ID}`\n\nAll resources tagged with: `AzureHayMaker-managed=true`\n\n---\n\n## Documentation References\n- [Azure Data Factory Quickstart](https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory)\n- [Copy Activity in Azure Data Factory](https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview)\n- [Azure SQL Database Overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/sql-database-paas-overview)\n- [Azure Storage Account with Data Lake Gen2](https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction)\n- [Azure Data Factory CLI Reference](https://learn.microsoft.com/en-us/cli/azure/datafactory)\n\n---\n\n## Automation Tool\n**Recommended**: Azure CLI\n\n**Rationale**: Azure CLI provides straightforward commands for all services in this scenario. While Terraform could be used for infrastructure, the Data Factory pipeline definitions are easier to manage with Azure CLI or ARM templates. For this scenario's complexity level, Azure CLI offers the best balance of simplicity and functionality.\n\n---\n\n## Estimated Duration\n- **Deployment**: 15-20 minutes\n- **Operations Phase**: 8 hours (with periodic data uploads and monitoring)\n- **Cleanup**: 5-10 minutes\n\n---\n\n## Notes\n- This scenario uses Azure SQL Database Basic/Standard tier to keep costs low\n- Data Factory pipeline definition is simplified - full implementation would include:\n  - Complete pipeline JSON with Copy Activity\n  - Schedule trigger configuration\n  - Error handling and retry logic\n- All operations scoped to single tenant and subscription\n- Connection strings stored in Key Vault following security best practices\n- Sample data files are minimal for testing purposes\n\n\n## Execution Plan\n\n### Phase 1: Data Collection\nGather required data from sources\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: data-ingestion, validation\n\n### Phase 2: Data Transformation\nTransform and clean data\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: parsing, transformation\n**Dependencies**: Data Collection\n\n### Phase 3: Data Analysis\nAnalyze processed data\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: analysis, pattern-detection\n**Dependencies**: Data Transformation\n\n### Phase 4: Report Generation\nGenerate results and reports\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: reporting, visualization\n**Dependencies**: Data Analysis\n\n## Success Criteria\n- Goal 'Scenario: Scheduled Batch ETL Pipeline...' is achieved\n\n## Instructions\nExecute the plan above autonomously:\n1. Follow each phase in sequence\n2. Use available skills and tools\n3. Verify success criteria are met\n4. Report progress and completion",
    "working_dir": ".",
    "sdk": "claude",
    "ui_mode": false,
    "success_criteria": [
      "Goal 'Scenario: Scheduled Batch ETL Pipeline...' is achieved"
    ],
    "constraints": []
  }
}

{
  "bundle_id": "749537b6-9479-4d01-b06d-fa742bb42809",
  "name": "data-scenario:-azure-databricks-agent",
  "version": "1.0.0",
  "metadata": {
    "domain": "data-processing",
    "complexity": "complex",
    "phase_count": 4,
    "skill_count": 2,
    "estimated_duration": "2 hours 36 minutes",
    "required_capabilities": [
      "reporting",
      "parsing",
      "data-ingestion",
      "visualization",
      "pattern-detection",
      "validation",
      "analysis",
      "transformation"
    ],
    "skill_names": [
      "data-processor",
      "documenter"
    ],
    "parallel_opportunities": 0,
    "risk_factors": [
      "High complexity may require extended execution time",
      "Large data volumes may cause performance issues"
    ]
  },
  "auto_mode_config": {
    "max_turns": 18,
    "initial_prompt": "# Goal: Scenario: Azure Databricks Cluster for Machine Learning\n\n## Objective\n# Scenario: Azure Databricks Cluster for Machine Learning\n\n## Technology Area\nAnalytics\n\n## Company Profile\n- **Company Size**: Large technology company\n- **Industry**: Technology / SaaS\n- **Use Case**: Build and train machine learning models using Apache Spark with collaborative notebooks\n\n## Scenario Description\nDeploy Azure Databricks with an Apache Spark cluster for collaborative data science work. Create notebooks, process large datasets using Spark, and prepare data for machine learning model training.\n\n## Azure Services Used\n- Azure Databricks\n- Azure Storage Account (data lake)\n- Azure Key Vault (credentials)\n- Azure Virtual Network (network isolation)\n\n## Prerequisites\n- Azure subscription with Contributor role\n- Azure CLI installed\n- A unique identifier for this scenario run\n\n---\n\n## Phase 1: Deployment and Validation\n\n### Environment Setup\n```bash\n# Set variables\nUNIQUE_ID=$(date +%Y%m%d%H%M%S)\nRESOURCE_GROUP=\"azurehaymaker-analytics-databricks-${UNIQUE_ID}-rg\"\nLOCATION=\"eastus\"\nDATABRICKS_WORKSPACE=\"azurehaymaker-dbk-${UNIQUE_ID}\"\nSTORAGE_ACCOUNT=\"azmkrdbk${UNIQUE_ID}\"\nKEYVAULT=\"azurehaymaker-kv-${UNIQUE_ID}\"\nVNET_NAME=\"azurehaymaker-vnet-${UNIQUE_ID}\"\n\n# Tags\nTAGS=\"AzureHayMaker-managed=true Scenario=analytics-databricks Owner=AzureHayMaker\"\n```\n\n### Deployment Steps\n```bash\n# Step 1: Create Resource Group\naz group create \\\n  --name \"${RESOURCE_GROUP}\" \\\n  --location \"${LOCATION}\" \\\n  --tags ${TAGS}\n\n# Step 2: Create Virtual Network for Databricks\naz network vnet create \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --name \"${VNET_NAME}\" \\\n  --address-prefix \"10.0.0.0/16\" \\\n  --subnet-name \"default\" \\\n  --subnet-prefixes \"10.0.0.0/24\" \\\n  --tags ${TAGS}\n\n# Step 3: Get VNet and subnet IDs\nVNET_ID=$(az network vnet show \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --name \"${VNET_NAME}\" \\\n  --query id -o tsv)\n\nSUBNET_ID=$(az network vnet subnet show \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --vnet-name \"${VNET_NAME}\" \\\n  --name \"default\" \\\n  --query id -o tsv)\n\n# Step 4: Create Storage Account for data\naz storage account create \\\n  --name \"${STORAGE_ACCOUNT}\" \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --location \"${LOCATION}\" \\\n  --sku Standard_LRS \\\n  --kind StorageV2 \\\n  --hierarchical-namespace true \\\n  --tags ${TAGS}\n\n# Step 5: Create containers\nSTORAGE_KEY=$(az storage account keys list \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --account-name \"${STORAGE_ACCOUNT}\" \\\n  --query '[0].value' -o tsv)\n\naz storage container create \\\n  --name \"datasets\" \\\n  --account-name \"${STORAGE_ACCOUNT}\" \\\n  --account-key \"${STORAGE_KEY}\"\n\naz storage container create \\\n  --name \"models\" \\\n  --account-name \"${STORAGE_ACCOUNT}\" \\\n  --account-key \"${STORAGE_KEY}\"\n\n# Step 6: Upload sample dataset\ncat > /tmp/sample_data.csv <<EOF\nfeature1,feature2,feature3,label\n1.5,2.3,0.5,1\n2.1,3.4,1.2,0\n1.8,2.9,0.8,1\n3.2,4.1,2.1,1\n0.9,1.5,0.3,0\nEOF\n\naz storage blob upload \\\n  --account-name \"${STORAGE_ACCOUNT}\" \\\n  --account-key \"${STORAGE_KEY}\" \\\n  --container-name \"datasets\" \\\n  --name \"sample_data.csv\" \\\n  --file /tmp/sample_data.csv\n\n# Step 7: Create Key Vault\naz keyvault create \\\n  --name \"${KEYVAULT}\" \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --location \"${LOCATION}\" \\\n  --tags ${TAGS}\n\naz keyvault secret set \\\n  --vault-name \"${KEYVAULT}\" \\\n  --name \"storage-key\" \\\n  --value \"${STORAGE_KEY}\"\n\n# Step 8: Create Databricks Workspace\naz databricks workspace create \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --name \"${DATABRICKS_WORKSPACE}\" \\\n  --location \"${LOCATION}\" \\\n  --sku premium \\\n  --tags ${TAGS}\n\n# Step 9: Create Databricks cluster configuration file\ncat > /tmp/cluster_config.json <<EOF\n{\n  \"spark_version\": \"13.3.x-scala2.12\",\n  \"node_type_id\": \"Standard_DS3_v2\",\n  \"num_workers\": 2,\n  \"spark_conf\": {\n    \"spark.databricks.cluster.profile\": \"singleNode\"\n  },\n  \"cluster_name\": \"ml-cluster\",\n  \"autotermination_minutes\": 30\n}\nEOF\n\n# Step 10: Get workspace URL for cluster creation context\nWORKSPACE_URL=$(az databricks workspace show \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --name \"${DATABRICKS_WORKSPACE}\" \\\n  --query \"workspaceUrl\" -o tsv)\n\necho \"\"\necho \"==========================================\"\necho \"Databricks Workspace Created: ${DATABRICKS_WORKSPACE}\"\necho \"Workspace URL: ${WORKSPACE_URL}\"\necho \"==========================================\"\n```\n\n### Validation\n```bash\n# Verify Resource Group\naz group show --name \"${RESOURCE_GROUP}\"\n\n# Verify Databricks Workspace\naz databricks workspace show \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --name \"${DATABRICKS_WORKSPACE}\"\n\n# Verify Storage Account\naz storage account show \\\n  --name \"${STORAGE_ACCOUNT}\" \\\n  --resource-group \"${RESOURCE_GROUP}\"\n\n# List storage containers\naz storage container list \\\n  --account-name \"${STORAGE_ACCOUNT}\" \\\n  --account-key \"${STORAGE_KEY}\" \\\n  --output table\n\n# Verify Key Vault\naz keyvault show --name \"${KEYVAULT}\"\n\n# Verify Virtual Network\naz network vnet show \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --name \"${VNET_NAME}\"\n\n# List all resources\naz resource list --resource-group \"${RESOURCE_GROUP}\" --output table\n```\n\n---\n\n## Phase 2: Mid-Day Operations and Management\n\n### Management Operations\n```bash\n# Operation 1: List Databricks workspaces\naz databricks workspace list --resource-group \"${RESOURCE_GROUP}\"\n\n# Operation 2: Monitor workspace resource usage\naz monitor metrics list \\\n  --resource \"/subscriptions/$(az account show --query id -o tsv)/resourceGroups/${RESOURCE_GROUP}/providers/Microsoft.Databricks/workspaces/${DATABRICKS_WORKSPACE}\" \\\n  --metric \"CPU%\" \\\n  --start-time $(date -u -d '1 hour ago' '+%Y-%m-%dT%H:%M:%SZ') \\\n  --end-time $(date -u '+%Y-%m-%dT%H:%M:%SZ')\n\n# Operation 3: Upload additional datasets\ncat > /tmp/training_data.csv <<EOF\nfeature1,feature2,feature3,feature4,label\n1.2,2.5,0.7,1.5,1\n2.3,3.6,1.4,2.1,0\n1.5,2.8,0.9,1.8,1\n3.4,4.5,2.3,2.8,1\n0.8,1.3,0.2,0.9,0\n1.9,3.1,1.0,2.0,1\nEOF\n\naz storage blob upload \\\n  --account-name \"${STORAGE_ACCOUNT}\" \\\n  --account-key \"${STORAGE_KEY}\" \\\n  --container-name \"datasets\" \\\n  --name \"training_data.csv\" \\\n  --file /tmp/training_data.csv \\\n  --overwrite\n\n# Operation 4: List all datasets\naz storage blob list \\\n  --account-name \"${STORAGE_ACCOUNT}\" \\\n  --account-key \"${STORAGE_KEY}\" \\\n  --container-name \"datasets\" \\\n  --output table\n\n# Operation 5: Check storage usage\naz storage account show-usage \\\n  --location \"${LOCATION}\"\n\n# Operation 6: View Key Vault secrets\naz keyvault secret list --vault-name \"${KEYVAULT}\" --output table\n\n# Operation 7: Create a notebook in Databricks context\necho \"To create notebooks, use Databricks UI at: ${WORKSPACE_URL}\"\necho \"Or use Databricks API with workspace token\"\n\n# Operation 8: List VNet and subnets\naz network vnet show \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --name \"${VNET_NAME}\" --output table\n```\n\n---\n\n## Phase 3: Cleanup and Tear-Down\n\n### Cleanup Steps\n```bash\n# Step 1: Delete Databricks Workspace\naz databricks workspace delete \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --name \"${DATABRICKS_WORKSPACE}\" \\\n  --yes\n\n# Step 2: Delete the entire resource group\naz group delete \\\n  --name \"${RESOURCE_GROUP}\" \\\n  --yes \\\n  --no-wait\n\n# Step 3: Verify deletion\nsleep 120\naz group exists --name \"${RESOURCE_GROUP}\"\n\n# Step 4: Confirm cleanup\necho \"Verifying cleanup...\"\naz resource list --resource-group \"${RESOURCE_GROUP}\" 2>&1 | grep \"could not be found\" && echo \"\u2713 Resource group successfully deleted\"\n\n# Step 5: Clean up local files\nrm -rf /tmp/sample_data.csv /tmp/training_data.csv /tmp/cluster_config.json\n```\n\n---\n\n## Resource Naming Convention\n- Resource Group: `azurehaymaker-analytics-databricks-${UNIQUE_ID}-rg`\n- Databricks Workspace: `azurehaymaker-dbk-${UNIQUE_ID}`\n- Storage Account: `azmkrdbk${UNIQUE_ID}`\n- Virtual Network: `azurehaymaker-vnet-${UNIQUE_ID}`\n- Key Vault: `azurehaymaker-kv-${UNIQUE_ID}`\n\nAll resources tagged with: `AzureHayMaker-managed=true`\n\n---\n\n## Documentation References\n- [Azure Databricks Overview](https://learn.microsoft.com/en-us/azure/databricks/introduction/)\n- [Create Databricks Workspace](https://learn.microsoft.com/en-us/azure/databricks/workspace/)\n- [Apache Spark in Databricks](https://learn.microsoft.com/en-us/azure/databricks/spark/)\n- [Databricks Clusters](https://learn.microsoft.com/en-us/azure/databricks/clusters/)\n- [Databricks CLI Reference](https://learn.microsoft.com/en-us/cli/azure/databricks)\n\n---\n\n## Automation Tool\n**Recommended**: Azure CLI\n\n**Rationale**: Azure CLI handles Databricks workspace creation efficiently. Advanced cluster and notebook management is typically done via Databricks UI or API after workspace provisioning.\n\n---\n\n## Estimated Duration\n- **Deployment**: 20-25 minutes (Databricks workspace provisioning takes time)\n- **Operations Phase**: 8 hours (with data uploads and cluster management)\n- **Cleanup**: 5-10 minutes\n\n---\n\n## Notes\n- Premium SKU provides features like job scheduling and DBFS\n- Spark clusters are created and managed within the Databricks workspace\n- All compute resources are ephemeral and created on-demand\n- Data stored in Azure Storage with direct Databricks integration\n- All operations scoped to single tenant and subscription\n- Notebooks support Python, SQL, Scala, and R languages\n\n\n## Execution Plan\n\n### Phase 1: Data Collection\nGather required data from sources\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: data-ingestion, validation\n\n### Phase 2: Data Transformation\nTransform and clean data\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: parsing, transformation\n**Dependencies**: Data Collection\n\n### Phase 3: Data Analysis\nAnalyze processed data\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: analysis, pattern-detection\n**Dependencies**: Data Transformation\n\n### Phase 4: Report Generation\nGenerate results and reports\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: reporting, visualization\n**Dependencies**: Data Analysis\n\n## Success Criteria\n- Goal 'Scenario: Azure Databricks Cluster for Machine Lea...' is achieved\n\n## Instructions\nExecute the plan above autonomously:\n1. Follow each phase in sequence\n2. Use available skills and tools\n3. Verify success criteria are met\n4. Report progress and completion",
    "working_dir": ".",
    "sdk": "claude",
    "ui_mode": false,
    "success_criteria": [
      "Goal 'Scenario: Azure Databricks Cluster for Machine Lea...' is achieved"
    ],
    "constraints": []
  }
}

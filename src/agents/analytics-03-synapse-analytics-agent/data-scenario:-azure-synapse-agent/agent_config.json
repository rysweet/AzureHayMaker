{
  "bundle_id": "1dda59cd-a181-41e7-a33a-608c4308eea4",
  "name": "data-scenario:-azure-synapse-agent",
  "version": "1.0.0",
  "metadata": {
    "domain": "data-processing",
    "complexity": "complex",
    "phase_count": 4,
    "skill_count": 2,
    "estimated_duration": "2 hours 36 minutes",
    "required_capabilities": [
      "analysis",
      "visualization",
      "transformation",
      "validation",
      "pattern-detection",
      "parsing",
      "data-ingestion",
      "reporting"
    ],
    "skill_names": [
      "data-processor",
      "documenter"
    ],
    "parallel_opportunities": 0,
    "risk_factors": [
      "High complexity may require extended execution time",
      "Large data volumes may cause performance issues"
    ]
  },
  "auto_mode_config": {
    "max_turns": 18,
    "initial_prompt": "# Goal: Scenario: Azure Synapse Analytics Workspace for Big Data\n\n## Objective\n# Scenario: Azure Synapse Analytics Workspace for Big Data\n\n## Technology Area\nAnalytics\n\n## Company Profile\n- **Company Size**: Large enterprise\n- **Industry**: Retail / E-commerce\n- **Use Case**: Unified analytics platform for data warehousing, big data analytics, and machine learning\n\n## Scenario Description\nDeploy Azure Synapse Analytics workspace with dedicated SQL pool, serverless SQL pool, and Spark pool for comprehensive data analytics. Process terabytes of data using either SQL or Spark, creating a unified analytics experience.\n\n## Azure Services Used\n- Azure Synapse Analytics (workspace)\n- Dedicated SQL Pool (data warehouse)\n- Serverless SQL Pool (on-demand querying)\n- Apache Spark Pool (big data processing)\n- Azure Storage Account (data lake)\n- Azure Key Vault (secrets)\n\n## Prerequisites\n- Azure subscription with Contributor role\n- Azure CLI installed\n- A unique identifier for this scenario run\n\n---\n\n## Phase 1: Deployment and Validation\n\n### Environment Setup\n```bash\n# Set variables\nUNIQUE_ID=$(date +%Y%m%d%H%M%S)\nRESOURCE_GROUP=\"azurehaymaker-analytics-synapse-${UNIQUE_ID}-rg\"\nLOCATION=\"eastus\"\nSYNAPSE_WORKSPACE=\"azurehaymaker-synapse-${UNIQUE_ID}\"\nSTORAGE_ACCOUNT=\"azmkrsynapse${UNIQUE_ID}\"\nSQL_POOL_NAME=\"sqldwpool\"\nSPARK_POOL_NAME=\"sparkpool\"\nKEYVAULT=\"azurehaymaker-kv-${UNIQUE_ID}\"\nSQL_ADMIN_USER=\"sqladmin\"\nSQL_ADMIN_PASSWORD=\"P@ssw0rd!${UNIQUE_ID}\"\n\n# Tags\nTAGS=\"AzureHayMaker-managed=true Scenario=analytics-synapse-analytics Owner=AzureHayMaker\"\n```\n\n### Deployment Steps\n```bash\n# Step 1: Create Resource Group\naz group create \\\n  --name \"${RESOURCE_GROUP}\" \\\n  --location \"${LOCATION}\" \\\n  --tags ${TAGS}\n\n# Step 2: Create Storage Account with hierarchical namespace\naz storage account create \\\n  --name \"${STORAGE_ACCOUNT}\" \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --location \"${LOCATION}\" \\\n  --sku Standard_LRS \\\n  --kind StorageV2 \\\n  --hierarchical-namespace true \\\n  --tags ${TAGS}\n\n# Step 3: Create file system for Synapse\nSTORAGE_KEY=$(az storage account keys list \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --account-name \"${STORAGE_ACCOUNT}\" \\\n  --query '[0].value' -o tsv)\n\naz storage fs create \\\n  --name \"synapse\" \\\n  --account-name \"${STORAGE_ACCOUNT}\" \\\n  --account-key \"${STORAGE_KEY}\"\n\naz storage fs create \\\n  --name \"data\" \\\n  --account-name \"${STORAGE_ACCOUNT}\" \\\n  --account-key \"${STORAGE_KEY}\"\n\n# Step 4: Create sample data\nmkdir -p /tmp/synapse-data\ncat > /tmp/synapse-data/sales.csv <<EOF\nproduct_id,date,quantity,revenue,region\nP001,2024-01-15,100,5000.00,North\nP002,2024-01-15,50,2500.00,South\nP001,2024-01-16,120,6000.00,East\nP003,2024-01-16,75,3750.00,West\nEOF\n\naz storage blob upload \\\n  --account-name \"${STORAGE_ACCOUNT}\" \\\n  --account-key \"${STORAGE_KEY}\" \\\n  --container-name \"data\" \\\n  --name \"sales.csv\" \\\n  --file /tmp/synapse-data/sales.csv\n\n# Step 5: Create Key Vault\naz keyvault create \\\n  --name \"${KEYVAULT}\" \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --location \"${LOCATION}\" \\\n  --tags ${TAGS}\n\naz keyvault secret set \\\n  --vault-name \"${KEYVAULT}\" \\\n  --name \"sql-admin-password\" \\\n  --value \"${SQL_ADMIN_PASSWORD}\"\n\n# Step 6: Create Synapse Workspace\naz synapse workspace create \\\n  --name \"${SYNAPSE_WORKSPACE}\" \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --storage-account \"${STORAGE_ACCOUNT}\" \\\n  --file-system \"synapse\" \\\n  --sql-admin-login-user \"${SQL_ADMIN_USER}\" \\\n  --sql-admin-login-password \"${SQL_ADMIN_PASSWORD}\" \\\n  --location \"${LOCATION}\" \\\n  --tags ${TAGS}\n\n# Step 7: Create Dedicated SQL Pool\naz synapse sql pool create \\\n  --workspace-name \"${SYNAPSE_WORKSPACE}\" \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --name \"${SQL_POOL_NAME}\" \\\n  --performance-level DW100c\n\n# Step 8: Create Spark Pool\naz synapse spark pool create \\\n  --workspace-name \"${SYNAPSE_WORKSPACE}\" \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --name \"${SPARK_POOL_NAME}\" \\\n  --spark-version 3.1 \\\n  --node-count 3 \\\n  --node-size Small \\\n  --enable-auto-scale false\n\n# Step 9: Configure workspace firewall\naz synapse workspace firewall-rule create \\\n  --workspace-name \"${SYNAPSE_WORKSPACE}\" \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --name \"AllowAllAzureIps\" \\\n  --start-ip-address \"0.0.0.0\" \\\n  --end-ip-address \"0.0.0.0\"\n\n# Step 10: Enable managed identity\naz synapse workspace update \\\n  --name \"${SYNAPSE_WORKSPACE}\" \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --assign-identity\n\necho \"\"\necho \"==========================================\"\necho \"Synapse Workspace Created: ${SYNAPSE_WORKSPACE}\"\necho \"SQL Pool: ${SQL_POOL_NAME}\"\necho \"Spark Pool: ${SPARK_POOL_NAME}\"\necho \"==========================================\"\n```\n\n### Validation\n```bash\n# Verify Resource Group\naz group show --name \"${RESOURCE_GROUP}\"\n\n# Verify Synapse Workspace\naz synapse workspace show \\\n  --name \"${SYNAPSE_WORKSPACE}\" \\\n  --resource-group \"${RESOURCE_GROUP}\"\n\n# List SQL pools\naz synapse sql pool list \\\n  --workspace-name \"${SYNAPSE_WORKSPACE}\" \\\n  --resource-group \"${RESOURCE_GROUP}\"\n\n# List Spark pools\naz synapse spark pool list \\\n  --workspace-name \"${SYNAPSE_WORKSPACE}\" \\\n  --resource-group \"${RESOURCE_GROUP}\"\n\n# Check workspace status\naz synapse workspace show \\\n  --name \"${SYNAPSE_WORKSPACE}\" \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --query \"connectivityEndpoints\" -o table\n\n# Verify storage\naz storage fs show \\\n  --name \"data\" \\\n  --account-name \"${STORAGE_ACCOUNT}\" \\\n  --account-key \"${STORAGE_KEY}\"\n\n# List all resources\naz resource list --resource-group \"${RESOURCE_GROUP}\" --output table\n```\n\n---\n\n## Phase 2: Mid-Day Operations and Management\n\n### Management Operations\n```bash\n# Operation 1: Pause Dedicated SQL Pool to save costs\naz synapse sql pool pause \\\n  --workspace-name \"${SYNAPSE_WORKSPACE}\" \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --name \"${SQL_POOL_NAME}\"\n\n# Operation 2: Resume Dedicated SQL Pool\naz synapse sql pool resume \\\n  --workspace-name \"${SYNAPSE_WORKSPACE}\" \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --name \"${SQL_POOL_NAME}\"\n\n# Operation 3: Scale SQL Pool\naz synapse sql pool update \\\n  --workspace-name \"${SYNAPSE_WORKSPACE}\" \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --name \"${SQL_POOL_NAME}\" \\\n  --performance-level DW200c\n\n# Operation 4: Check Spark pool status\naz synapse spark pool show \\\n  --workspace-name \"${SYNAPSE_WORKSPACE}\" \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --name \"${SPARK_POOL_NAME}\" \\\n  --query \"nodeCount\" -o tsv\n\n# Operation 5: Monitor SQL Pool metrics\naz monitor metrics list \\\n  --resource \"/subscriptions/$(az account show --query id -o tsv)/resourceGroups/${RESOURCE_GROUP}/providers/Microsoft.Synapse/workspaces/${SYNAPSE_WORKSPACE}/sqlPools/${SQL_POOL_NAME}\" \\\n  --metric \"CPU%\" \\\n  --start-time $(date -u -d '1 hour ago' '+%Y-%m-%dT%H:%M:%SZ') \\\n  --end-time $(date -u '+%Y-%m-%dT%H:%M:%SZ')\n\n# Operation 6: Upload additional data\ncat > /tmp/synapse-data/inventory.csv <<EOF\nproduct_id,warehouse_id,quantity_on_hand,reorder_level\nP001,W1,500,100\nP002,W2,300,75\nP003,W1,200,50\nEOF\n\naz storage blob upload \\\n  --account-name \"${STORAGE_ACCOUNT}\" \\\n  --account-key \"${STORAGE_KEY}\" \\\n  --container-name \"data\" \\\n  --name \"inventory.csv\" \\\n  --file /tmp/synapse-data/inventory.csv \\\n  --overwrite\n\n# Operation 7: List data files\naz storage blob list \\\n  --account-name \"${STORAGE_ACCOUNT}\" \\\n  --account-key \"${STORAGE_KEY}\" \\\n  --container-name \"data\" \\\n  --output table\n\n# Operation 8: Monitor workspace activity\naz synapse workspace show \\\n  --name \"${SYNAPSE_WORKSPACE}\" \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --query \"connectivity\" -o table\n```\n\n---\n\n## Phase 3: Cleanup and Tear-Down\n\n### Cleanup Steps\n```bash\n# Step 1: Pause SQL pool before deletion\naz synapse sql pool pause \\\n  --workspace-name \"${SYNAPSE_WORKSPACE}\" \\\n  --resource-group \"${RESOURCE_GROUP}\" \\\n  --name \"${SQL_POOL_NAME}\" \\\n  --no-wait\n\n# Step 2: Wait for pause to complete\nsleep 60\n\n# Step 3: Delete the entire resource group\naz group delete \\\n  --name \"${RESOURCE_GROUP}\" \\\n  --yes \\\n  --no-wait\n\n# Step 4: Verify deletion\nsleep 120\naz group exists --name \"${RESOURCE_GROUP}\"\n\n# Step 5: Confirm cleanup\necho \"Verifying cleanup...\"\naz resource list --resource-group \"${RESOURCE_GROUP}\" 2>&1 | grep \"could not be found\" && echo \"\u2713 Resource group successfully deleted\"\n\n# Step 6: Clean up local files\nrm -rf /tmp/synapse-data\n```\n\n---\n\n## Resource Naming Convention\n- Resource Group: `azurehaymaker-analytics-synapse-${UNIQUE_ID}-rg`\n- Synapse Workspace: `azurehaymaker-synapse-${UNIQUE_ID}`\n- Storage Account: `azmkrsynapse${UNIQUE_ID}`\n- SQL Pool: `sqldwpool`\n- Spark Pool: `sparkpool`\n- Key Vault: `azurehaymaker-kv-${UNIQUE_ID}`\n\nAll resources tagged with: `AzureHayMaker-managed=true`\n\n---\n\n## Documentation References\n- [Azure Synapse Analytics Overview](https://learn.microsoft.com/en-us/azure/synapse-analytics/overview-what-is)\n- [Synapse Workspace Guide](https://learn.microsoft.com/en-us/azure/synapse-analytics/quickstart-create-workspace)\n- [Dedicated SQL Pool](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/overview)\n- [Apache Spark in Synapse](https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-overview)\n- [Synapse CLI Reference](https://learn.microsoft.com/en-us/cli/azure/synapse)\n\n---\n\n## Automation Tool\n**Recommended**: Azure CLI\n\n**Rationale**: Azure CLI provides straightforward commands for all Synapse components. While Terraform and Bicep are viable, Azure CLI is most efficient for rapid deployment and management.\n\n---\n\n## Estimated Duration\n- **Deployment**: 20-30 minutes (SQL and Spark pools take time to provision)\n- **Operations Phase**: 8 hours (with scaling, monitoring, and data uploads)\n- **Cleanup**: 10-15 minutes\n\n---\n\n## Notes\n- Dedicated SQL Pool can be paused to save costs when not in use\n- Spark pools provide distributed computing for big data\n- Serverless SQL pool queries data directly from storage without provisioned resources\n- Managed identity enables secure access to storage and Key Vault\n- All operations scoped to single tenant and subscription\n- Data lake storage provides scalable, performant data storage\n\n\n## Execution Plan\n\n### Phase 1: Data Collection\nGather required data from sources\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: data-ingestion, validation\n\n### Phase 2: Data Transformation\nTransform and clean data\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: parsing, transformation\n**Dependencies**: Data Collection\n\n### Phase 3: Data Analysis\nAnalyze processed data\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: analysis, pattern-detection\n**Dependencies**: Data Transformation\n\n### Phase 4: Report Generation\nGenerate results and reports\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: reporting, visualization\n**Dependencies**: Data Analysis\n\n## Success Criteria\n- Goal 'Scenario: Azure Synapse Analytics Workspace for Bi...' is achieved\n\n## Instructions\nExecute the plan above autonomously:\n1. Follow each phase in sequence\n2. Use available skills and tools\n3. Verify success criteria are met\n4. Report progress and completion",
    "working_dir": ".",
    "sdk": "claude",
    "ui_mode": false,
    "success_criteria": [
      "Goal 'Scenario: Azure Synapse Analytics Workspace for Bi...' is achieved"
    ],
    "constraints": []
  }
}

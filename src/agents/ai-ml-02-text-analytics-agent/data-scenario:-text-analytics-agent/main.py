#!/usr/bin/env python3
"""
data-scenario:-text-analytics-agent - Autonomous Goal-Seeking Agent

Generated by Amplihack Goal Agent Generator
"""

import sys
from pathlib import Path

# Add parent directory to path for amplihack imports
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    from amplihack.launcher.auto_mode import AutoMode
except ImportError:
    print("Error: amplihack package not found")
    print("Install with: pip install amplihack")
    sys.exit(1)


def main():
    """Execute the goal-seeking agent."""
    # Load configuration
    config = {
        "max_turns": 18,
        "initial_prompt": '# Goal: Scenario: Text Analytics for Sentiment Analysis\n\n## Objective\n# Scenario: Text Analytics for Sentiment Analysis\n\n## Technology Area\nAI & ML\n\n## Company Profile\n- **Company Size**: Mid-size tech company\n- **Industry**: Customer Service / SaaS Platform\n- **Use Case**: Analyze customer feedback and support tickets for sentiment to prioritize high-value support cases and identify trends\n\n## Scenario Description\nDeploy Azure Cognitive Services Text Analytics to analyze customer feedback, support tickets, and reviews for sentiment scores and key phrases. This scenario demonstrates text processing, sentiment classification, and operational management of language AI services.\n\n## Azure Services Used\n- Azure Cognitive Services (Text Analytics)\n- Azure Storage Account (for text data storage)\n- Azure Cosmos DB (for storing analysis results)\n- Azure Key Vault (for API credentials)\n\n## Prerequisites\n- Azure subscription with Contributor role\n- Azure CLI installed and configured\n- Sample text data for analysis\n- cURL or similar tool for API calls\n\n---\n\n## Phase 1: Deployment and Validation\n\n### Environment Setup\n```bash\n# Set variables\nUNIQUE_ID=$(date +%Y%m%d%H%M%S)\nRESOURCE_GROUP="azurehaymaker-textanalytics-${UNIQUE_ID}-rg"\nLOCATION="eastus"\nTEXT_RESOURCE="azurehaymaker-text-${UNIQUE_ID}"\nSTORAGE_ACCOUNT="azhmaktext${UNIQUE_ID}"\nCOSMOS_ACCOUNT="azurehaymaker-cosmos-${UNIQUE_ID}"\nKEYVAULT="azurehaymaker-kv-${UNIQUE_ID}"\nCOSMOS_DB="sentiment-analysis"\n\n# Tags\nTAGS="AzureHayMaker-managed=true Scenario=ai-ml-text-analytics Owner=AzureHayMaker"\n```\n\n### Deployment Steps\n```bash\n# Step 1: Create Resource Group\naz group create \\\n  --name "${RESOURCE_GROUP}" \\\n  --location "${LOCATION}" \\\n  --tags ${TAGS}\n\n# Step 2: Create Azure Cognitive Services Text Analytics\naz cognitiveservices account create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${TEXT_RESOURCE}" \\\n  --kind TextAnalytics \\\n  --sku S0 \\\n  --location "${LOCATION}" \\\n  --custom-domain "${TEXT_RESOURCE}" \\\n  --tags ${TAGS}\n\n# Step 3: Get Text Analytics endpoint and key\nTEXT_ENDPOINT=$(az cognitiveservices account show \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${TEXT_RESOURCE}" \\\n  --query "properties.endpoint" -o tsv)\n\nTEXT_KEY=$(az cognitiveservices account keys list \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${TEXT_RESOURCE}" \\\n  --query "key1" -o tsv)\n\necho "Text Analytics Endpoint: ${TEXT_ENDPOINT}"\necho "Text Analytics Key: ${TEXT_KEY}"\n\n# Step 4: Create Storage Account for text data storage\naz storage account create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${STORAGE_ACCOUNT}" \\\n  --location "${LOCATION}" \\\n  --sku Standard_LRS \\\n  --kind StorageV2 \\\n  --tags ${TAGS}\n\n# Step 5: Create storage containers\nSTORAGE_KEY=$(az storage account keys list \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --account-name "${STORAGE_ACCOUNT}" \\\n  --query \'[0].value\' -o tsv)\n\naz storage container create \\\n  --name "feedback-data" \\\n  --account-name "${STORAGE_ACCOUNT}" \\\n  --account-key "${STORAGE_KEY}" \\\n  --public-access off\n\naz storage container create \\\n  --name "analysis-results" \\\n  --account-name "${STORAGE_ACCOUNT}" \\\n  --account-key "${STORAGE_KEY}" \\\n  --public-access off\n\n# Step 6: Create Cosmos DB account for storing results\naz cosmosdb create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${COSMOS_ACCOUNT}" \\\n  --locations regionName="${LOCATION}" isZoneRedundant=false failoverPriority=0 \\\n  --capabilities EnableServerless \\\n  --kind GlobalDocumentDB \\\n  --tags ${TAGS}\n\n# Step 7: Create Cosmos DB database\naz cosmosdb sql database create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --account-name "${COSMOS_ACCOUNT}" \\\n  --name "${COSMOS_DB}"\n\n# Step 8: Create Cosmos DB container for sentiment results\naz cosmosdb sql container create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --account-name "${COSMOS_ACCOUNT}" \\\n  --database-name "${COSMOS_DB}" \\\n  --name "sentiment-results" \\\n  --partition-key-path "/id"\n\n# Step 9: Create Key Vault and store credentials\naz keyvault create \\\n  --name "${KEYVAULT}" \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --location "${LOCATION}" \\\n  --tags ${TAGS}\n\naz keyvault secret set \\\n  --vault-name "${KEYVAULT}" \\\n  --name "text-endpoint" \\\n  --value "${TEXT_ENDPOINT}"\n\naz keyvault secret set \\\n  --vault-name "${KEYVAULT}" \\\n  --name "text-key" \\\n  --value "${TEXT_KEY}"\n\n# Step 10: Upload sample feedback data\ncat > /tmp/customer-feedback.json << EOF\n{\n  "documents": [\n    {\n      "id": "1",\n      "language": "en",\n      "text": "This product is absolutely amazing! I love it and recommend to everyone."\n    },\n    {\n      "id": "2",\n      "language": "en",\n      "text": "Terrible experience. The service was slow and unhelpful. Very disappointed."\n    },\n    {\n      "id": "3",\n      "language": "en",\n      "text": "Good product overall. Works as expected. Some minor issues but acceptable."\n    }\n  ]\n}\nEOF\n\naz storage blob upload \\\n  --account-name "${STORAGE_ACCOUNT}" \\\n  --account-key "${STORAGE_KEY}" \\\n  --container-name "feedback-data" \\\n  --name "feedback-batch-001.json" \\\n  --file /tmp/customer-feedback.json\n```\n\n### Validation\n```bash\n# Verify Resource Group\naz group show --name "${RESOURCE_GROUP}"\n\n# Verify Text Analytics Resource\naz cognitiveservices account show \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${TEXT_RESOURCE}"\n\n# Verify Storage Account\naz storage account show --name "${STORAGE_ACCOUNT}"\n\n# Verify Cosmos DB Account\naz cosmosdb show \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${COSMOS_ACCOUNT}"\n\n# Verify Cosmos DB Database and Container\naz cosmosdb sql database show \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --account-name "${COSMOS_ACCOUNT}" \\\n  --name "${COSMOS_DB}"\n\naz cosmosdb sql container show \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --account-name "${COSMOS_ACCOUNT}" \\\n  --database-name "${COSMOS_DB}" \\\n  --name "sentiment-results"\n\n# Verify Key Vault\naz keyvault show --name "${KEYVAULT}"\n\n# List all resources\naz resource list --resource-group "${RESOURCE_GROUP}" --output table\n\n# Test Text Analytics API connectivity\ncurl -I -X GET "${TEXT_ENDPOINT}text/analytics/v3.1/languages" \\\n  -H "Ocp-Apim-Subscription-Key: ${TEXT_KEY}"\n```\n\n---\n\n## Phase 2: Mid-Day Operations and Management\n\n### Management Operations\n```bash\n# Operation 1: Analyze sentiment of customer feedback\ncurl -X POST "${TEXT_ENDPOINT}text/analytics/v3.1/sentiment?model-version=latest&showStats=true" \\\n  -H "Content-Type: application/json" \\\n  -H "Ocp-Apim-Subscription-Key: ${TEXT_KEY}" \\\n  -d @/tmp/customer-feedback.json \\\n  | jq \'.\' > /tmp/sentiment-results.json\n\ncat /tmp/sentiment-results.json\n\n# Operation 2: Extract key phrases from feedback\ncurl -X POST "${TEXT_ENDPOINT}text/analytics/v3.1/keyPhrases" \\\n  -H "Content-Type: application/json" \\\n  -H "Ocp-Apim-Subscription-Key: ${TEXT_KEY}" \\\n  -d @/tmp/customer-feedback.json \\\n  | jq \'.documents[] | {id, keyPhrases}\'\n\n# Operation 3: Detect language of text documents\ncurl -X POST "${TEXT_ENDPOINT}text/analytics/v3.1/languages" \\\n  -H "Content-Type: application/json" \\\n  -H "Ocp-Apim-Subscription-Key: ${TEXT_KEY}" \\\n  -d @/tmp/customer-feedback.json \\\n  | jq \'.\'\n\n# Operation 4: Extract named entities\ncurl -X POST "${TEXT_ENDPOINT}text/analytics/v3.1/entities/recognition/general" \\\n  -H "Content-Type: application/json" \\\n  -H "Ocp-Apim-Subscription-Key: ${TEXT_KEY}" \\\n  -d @/tmp/customer-feedback.json \\\n  | jq \'.documents[] | {id, entities}\'\n\n# Operation 5: Store analysis results in Cosmos DB\nCOSMOS_KEY=$(az cosmosdb keys list \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${COSMOS_ACCOUNT}" \\\n  --type keys \\\n  --query primaryMasterKey -o tsv)\n\ncat > /tmp/cosmos-insert.json << EOF\n{\n  "id": "sentiment-analysis-${UNIQUE_ID}",\n  "timestamp": "$(date -u \'+%Y-%m-%dT%H:%M:%SZ\')",\n  "documentsAnalyzed": 3,\n  "sentimentDistribution": {\n    "positive": 1,\n    "negative": 1,\n    "neutral": 1\n  }\n}\nEOF\n\n# Operation 6: Upload processed results to storage\naz storage blob upload \\\n  --account-name "${STORAGE_ACCOUNT}" \\\n  --account-key "${STORAGE_KEY}" \\\n  --container-name "analysis-results" \\\n  --name "sentiment-${UNIQUE_ID}.json" \\\n  --file /tmp/sentiment-results.json\n\n# Operation 7: List all feedback files processed\naz storage blob list \\\n  --account-name "${STORAGE_ACCOUNT}" \\\n  --account-key "${STORAGE_KEY}" \\\n  --container-name "feedback-data" \\\n  --output table\n\n# Operation 8: Get Text Analytics resource quota and usage\naz cognitiveservices account show \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${TEXT_RESOURCE}" \\\n  --query "{Name: name, Kind: kind, Sku: sku.name, Endpoint: properties.endpoint}"\n\n# Operation 9: Monitor Text Analytics metrics\naz monitor metrics list \\\n  --resource "/subscriptions/$(az account show --query id -o tsv)/resourceGroups/${RESOURCE_GROUP}/providers/Microsoft.CognitiveServices/accounts/${TEXT_RESOURCE}" \\\n  --metric "SuccessfulRequests" \\\n  --start-time $(date -u -d \'1 hour ago\' \'+%Y-%m-%dT%H:%M:%SZ\') \\\n  --end-time $(date -u \'+%Y-%m-%dT%H:%M:%SZ\')\n\n# Operation 10: Add new customer feedback for analysis\ncat > /tmp/new-feedback.json << EOF\n{\n  "documents": [\n    {\n      "id": "4",\n      "language": "en",\n      "text": "Outstanding customer service and excellent product quality!"\n    }\n  ]\n}\nEOF\n\ncurl -X POST "${TEXT_ENDPOINT}text/analytics/v3.1/sentiment" \\\n  -H "Content-Type: application/json" \\\n  -H "Ocp-Apim-Subscription-Key: ${TEXT_KEY}" \\\n  -d @/tmp/new-feedback.json | jq \'.documents[0] | {id, sentiment, scores}\'\n\n# Operation 11: Check Cosmos DB item count\nCOSMOS_URI="https://${COSMOS_ACCOUNT}.documents.azure.com:443/"\necho "Cosmos DB connection string generated for: ${COSMOS_URI}"\n\n# Operation 12: Retrieve and display analysis from storage\naz storage blob list \\\n  --account-name "${STORAGE_ACCOUNT}" \\\n  --account-key "${STORAGE_KEY}" \\\n  --container-name "analysis-results" \\\n  --output table\n```\n\n---\n\n## Phase 3: Cleanup and Tear-Down\n\n### Cleanup Steps\n```bash\n# Step 1: Delete the entire resource group (includes all resources)\naz group delete \\\n  --name "${RESOURCE_GROUP}" \\\n  --yes \\\n  --no-wait\n\n# Step 2: Wait for deletion to complete\necho "Waiting for resource group deletion..."\nsleep 120\n\n# Step 3: Verify deletion\naz group exists --name "${RESOURCE_GROUP}"\n\n# Step 4: Confirm cleanup\necho "Verifying cleanup..."\naz resource list --resource-group "${RESOURCE_GROUP}" 2>&1 | grep "could not be found" && echo "âœ“ Resource group successfully deleted"\n```\n\n---\n\n## Resource Naming Convention\n- Resource Group: `azurehaymaker-textanalytics-${UNIQUE_ID}-rg`\n- Text Analytics: `azurehaymaker-text-${UNIQUE_ID}`\n- Storage Account: `azhmaktext${UNIQUE_ID}`\n- Cosmos DB: `azurehaymaker-cosmos-${UNIQUE_ID}`\n- Key Vault: `azurehaymaker-kv-${UNIQUE_ID}`\n\nAll resources tagged with: `AzureHayMaker-managed=true`\n\n---\n\n## Documentation References\n- [Azure Text Analytics Documentation](https://learn.microsoft.com/en-us/azure/cognitive-services/text-analytics/)\n- [Text Analytics API Reference](https://learn.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-call-api)\n- [Sentiment Analysis Guide](https://learn.microsoft.com/en-us/azure/cognitive-services/text-analytics/concepts/sentiment-analysis)\n- [Key Phrase Extraction](https://learn.microsoft.com/en-us/azure/cognitive-services/text-analytics/concepts/key-phrase-extraction)\n- [Named Entity Recognition](https://learn.microsoft.com/en-us/azure/cognitive-services/text-analytics/concepts/named-entity-recognition)\n- [Azure Cosmos DB Overview](https://learn.microsoft.com/en-us/azure/cosmos-db/introduction)\n\n---\n\n## Automation Tool\n**Recommended**: Azure CLI with REST API calls\n\n**Rationale**: Azure CLI manages infrastructure provisioning efficiently, while REST API calls via curl provide flexible text analysis operations. This combination gives straightforward access to all Text Analytics capabilities.\n\n---\n\n## Estimated Duration\n- **Deployment**: 15-20 minutes\n- **Operations Phase**: 8+ hours (with multiple analyses, data uploads, and result storage)\n- **Cleanup**: 5-10 minutes\n\n---\n\n## Notes\n- Text Analytics supports multiple languages and language detection\n- Sentiment analysis returns scores from 0 (negative) to 1 (positive)\n- Key phrases are automatically extracted from input text\n- Cosmos DB stores analysis results in a scalable, distributed manner\n- All operations scoped to single tenant and subscription\n- Results include confidence scores for each classification\n- Document size limited to 5,120 characters per document\n\n\n## Execution Plan\n\n### Phase 1: Data Collection\nGather required data from sources\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: data-ingestion, validation\n\n### Phase 2: Data Transformation\nTransform and clean data\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: parsing, transformation\n**Dependencies**: Data Collection\n\n### Phase 3: Data Analysis\nAnalyze processed data\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: analysis, pattern-detection\n**Dependencies**: Data Transformation\n\n### Phase 4: Report Generation\nGenerate results and reports\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: reporting, visualization\n**Dependencies**: Data Analysis\n\n## Success Criteria\n- Goal \'Scenario: Text Analytics for Sentiment Analysis...\' is achieved\n\n## Instructions\nExecute the plan above autonomously:\n1. Follow each phase in sequence\n2. Use available skills and tools\n3. Verify success criteria are met\n4. Report progress and completion',
        "working_dir": ".",
        "sdk": "claude",
        "ui_mode": False,
        "success_criteria": [
            "Goal 'Scenario: Text Analytics for Sentiment Analysis...' is achieved"
        ],
        "constraints": [],
    }

    # Read initial prompt
    prompt_path = Path(__file__).parent / "prompt.md"
    if not prompt_path.exists():
        print("Error: prompt.md not found")
        sys.exit(1)

    initial_prompt = prompt_path.read_text()

    # Create auto-mode instance
    auto_mode = AutoMode(
        sdk=config.get("sdk", "claude"),
        prompt=initial_prompt,
        max_turns=config.get("max_turns", 10),
        working_dir=Path(__file__).parent,
        ui_mode=config.get("ui_mode", False),
    )

    # Run agent
    print("Starting data-scenario:-text-analytics-agent...")
    print("Goal: Scenario: Text Analytics for Sentiment Analysis")
    print("Estimated duration: 2 hours 36 minutes")
    print()

    exit_code = auto_mode.run()

    if exit_code == 0:
        print("\nGoal achieved successfully!")
    else:
        print(f"\nGoal execution failed with code {exit_code}")

    return exit_code


if __name__ == "__main__":
    sys.exit(main())

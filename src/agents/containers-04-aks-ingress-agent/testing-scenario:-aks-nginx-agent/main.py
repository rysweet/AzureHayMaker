#!/usr/bin/env python3
"""
testing-scenario:-aks-nginx-agent - Autonomous Goal-Seeking Agent

Generated by Amplihack Goal Agent Generator
"""

import sys
from pathlib import Path

# Add parent directory to path for amplihack imports
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    from amplihack.launcher.auto_mode import AutoMode
except ImportError:
    print("Error: amplihack package not found")
    print("Install with: pip install amplihack")
    sys.exit(1)


def main():
    """Execute the goal-seeking agent."""
    # Load configuration
    config = {
        "max_turns": 18,
        "initial_prompt": '# Goal: Scenario: AKS with NGINX Ingress Controller\n\n## Objective\n# Scenario: AKS with NGINX Ingress Controller\n\n## Technology Area\nContainers\n\n## Company Profile\n- **Company Size**: Mid-size enterprise\n- **Industry**: Financial Services / SaaS\n- **Use Case**: Route external traffic to multiple microservices with SSL/TLS termination\n\n## Scenario Description\nDeploy AKS cluster with NGINX ingress controller for advanced routing capabilities. Configure ingress rules for multiple applications, implement SSL/TLS termination, and set up hostname-based routing.\n\n## Azure Services Used\n- Azure Kubernetes Service (AKS)\n- Azure Container Registry (image storage)\n- NGINX Ingress Controller (traffic routing)\n- Azure Public IP (external access)\n- Azure Key Vault (certificate storage)\n\n## Prerequisites\n- Azure subscription with Contributor role\n- Azure CLI installed with kubectl and helm\n- Docker installed\n- A unique identifier for this scenario run\n\n---\n\n## Phase 1: Deployment and Validation\n\n### Environment Setup\n```bash\n# Set variables\nUNIQUE_ID=$(date +%Y%m%d%H%M%S)\nRESOURCE_GROUP="azurehaymaker-containers-ingress-${UNIQUE_ID}-rg"\nLOCATION="eastus"\nAKS_CLUSTER="azurehaymaker-aks-ingress-${UNIQUE_ID}"\nACR_REGISTRY="azmkringress${UNIQUE_ID}"\nVNET_NAME="azurehaymaker-vnet-${UNIQUE_ID}"\nKEYVAULT="azurehaymaker-kv-${UNIQUE_ID}"\nLOG_ANALYTICS="azurehaymaker-logs-${UNIQUE_ID}"\n\n# Tags\nTAGS="AzureHayMaker-managed=true Scenario=containers-aks-ingress Owner=AzureHayMaker"\n```\n\n### Deployment Steps\n```bash\n# Step 1: Create Resource Group\naz group create \\\n  --name "${RESOURCE_GROUP}" \\\n  --location "${LOCATION}" \\\n  --tags ${TAGS}\n\n# Step 2: Create Virtual Network\naz network vnet create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${VNET_NAME}" \\\n  --address-prefix "10.0.0.0/16" \\\n  --subnet-name "aks-subnet" \\\n  --subnet-prefixes "10.0.0.0/22" \\\n  --tags ${TAGS}\n\nSUBNET_ID=$(az network vnet subnet show \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --vnet-name "${VNET_NAME}" \\\n  --name "aks-subnet" \\\n  --query id -o tsv)\n\n# Step 3: Create Azure Container Registry\naz acr create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${ACR_REGISTRY}" \\\n  --sku Standard \\\n  --admin-enabled true \\\n  --tags ${TAGS}\n\n# Step 4: Create Log Analytics Workspace\naz monitor log-analytics workspace create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --workspace-name "${LOG_ANALYTICS}" \\\n  --location "${LOCATION}" \\\n  --tags ${TAGS}\n\nLOG_ANALYTICS_ID=$(az monitor log-analytics workspace show \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --workspace-name "${LOG_ANALYTICS}" \\\n  --query customerId -o tsv)\n\nLOG_ANALYTICS_KEY=$(az monitor log-analytics workspace get-shared-keys \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --workspace-name "${LOG_ANALYTICS}" \\\n  --query primarySharedKey -o tsv)\n\n# Step 5: Create Key Vault\naz keyvault create \\\n  --name "${KEYVAULT}" \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --location "${LOCATION}" \\\n  --tags ${TAGS}\n\n# Step 6: Create AKS Cluster\naz aks create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${AKS_CLUSTER}" \\\n  --location "${LOCATION}" \\\n  --node-count 2 \\\n  --vm-set-type VirtualMachineScaleSets \\\n  --load-balancer-sku standard \\\n  --enable-managed-identity \\\n  --network-plugin azure \\\n  --vnet-subnet-id "${SUBNET_ID}" \\\n  --docker-bridge-address "172.17.0.1/16" \\\n  --service-cidr "10.1.0.0/16" \\\n  --dns-service-ip "10.1.0.10" \\\n  --enable-addons monitoring \\\n  --workspace-resource-id "/subscriptions/$(az account show --query id -o tsv)/resourcegroups/${RESOURCE_GROUP}/providers/microsoft.operationalinsights/workspaces/${LOG_ANALYTICS}" \\\n  --enable-cluster-autoscaling \\\n  --min-count 1 \\\n  --max-count 5 \\\n  --attach-acr "${ACR_REGISTRY}" \\\n  --tags ${TAGS}\n\n# Step 7: Get credentials\naz aks get-credentials \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${AKS_CLUSTER}" \\\n  --overwrite-existing\n\n# Step 8: Create namespace for ingress controller\nkubectl create namespace ingress-nginx\n\n# Step 9: Add Helm repository and install NGINX Ingress Controller\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\n\nhelm install nginx-ingress ingress-nginx/ingress-nginx \\\n  --namespace ingress-nginx \\\n  --set controller.service.type=LoadBalancer \\\n  --set controller.service.externalTrafficPolicy=Local\n\n# Step 10: Build sample applications\nACR_LOGIN_SERVER=$(az acr show \\\n  --name "${ACR_REGISTRY}" \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --query loginServer -o tsv)\n\nfor APP_NAME in app-blue app-red; do\n  cat > /tmp/Dockerfile_${APP_NAME} <<EOF\nFROM nginx:alpine\nRUN echo \'<html><body><h1>${APP_NAME}</h1><p>Ingress routing demo</p></body></html>\' > /usr/share/nginx/html/index.html\nEOF\n\n  az acr build \\\n    --registry "${ACR_REGISTRY}" \\\n    --image "${APP_NAME}:v1" \\\n    --file /tmp/Dockerfile_${APP_NAME} \\\n    /tmp/\ndone\n\n# Step 11: Create application deployments\ncat > /tmp/apps_deployment.yaml <<EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-blue\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: app-blue\n  template:\n    metadata:\n      labels:\n        app: app-blue\n    spec:\n      containers:\n      - name: app-blue\n        image: ${ACR_LOGIN_SERVER}/app-blue:v1\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: app-blue-service\nspec:\n  selector:\n    app: app-blue\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n  type: ClusterIP\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-red\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: app-red\n  template:\n    metadata:\n      labels:\n        app: app-red\n    spec:\n      containers:\n      - name: app-red\n        image: ${ACR_LOGIN_SERVER}/app-red:v1\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: app-red-service\nspec:\n  selector:\n    app: app-red\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n  type: ClusterIP\nEOF\n\nkubectl apply -f /tmp/apps_deployment.yaml\n\n# Step 12: Create ingress resource\ncat > /tmp/ingress.yaml <<EOF\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: main-ingress\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  rules:\n  - host: app-blue.example.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: app-blue-service\n            port:\n              number: 80\n  - host: app-red.example.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: app-red-service\n            port:\n              number: 80\nEOF\n\nkubectl apply -f /tmp/ingress.yaml\n\necho ""\necho "=========================================="\necho "AKS Cluster with Ingress: ${AKS_CLUSTER}"\necho "NGINX Ingress installed in namespace: ingress-nginx"\necho "=========================================="\n```\n\n### Validation\n```bash\n# Verify AKS Cluster\naz aks show \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${AKS_CLUSTER}"\n\n# Check nodes\nkubectl get nodes -o wide\n\n# Get NGINX ingress controller service\nkubectl get svc -n ingress-nginx\n\n# Get external IP of ingress\nINGRESS_IP=$(kubectl get svc -n ingress-nginx nginx-ingress-ingress-nginx-controller -o jsonpath=\'{.status.loadBalancer.ingress[0].ip}\')\necho "Ingress External IP: ${INGRESS_IP}"\n\n# Check deployments\nkubectl get deployments -o wide\n\n# Check services\nkubectl get svc -o wide\n\n# Check ingress rules\nkubectl get ingress -o wide\n\n# Check pods\nkubectl get pods -o wide\n\n# View ingress details\nkubectl describe ingress main-ingress\n\n# List all resources\naz resource list --resource-group "${RESOURCE_GROUP}" --output table\n```\n\n---\n\n## Phase 2: Mid-Day Operations and Management\n\n### Management Operations\n```bash\n# Operation 1: Scale application deployments\nkubectl scale deployment app-blue --replicas=3\nkubectl scale deployment app-red --replicas=3\n\n# Operation 2: Check pods across deployments\nkubectl get pods -o wide\n\n# Operation 3: View ingress controller logs\nkubectl logs -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx --tail=50\n\n# Operation 4: Create new ingress rule\ncat > /tmp/new_ingress.yaml <<EOF\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: api-ingress\n  annotations:\n    kubernetes.io/ingress.class: nginx\nspec:\n  rules:\n  - host: api.example.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: app-blue-service\n            port:\n              number: 80\nEOF\n\nkubectl apply -f /tmp/new_ingress.yaml\n\n# Operation 5: Get all ingress rules\nkubectl get ingress -A\n\n# Operation 6: Monitor ingress metrics\naz monitor metrics list \\\n  --resource "/subscriptions/$(az account show --query id -o tsv)/resourceGroups/${RESOURCE_GROUP}/providers/Microsoft.ContainerService/managedClusters/${AKS_CLUSTER}" \\\n  --metric "node_network_in_bytes" \\\n  --start-time $(date -u -d \'1 hour ago\' \'+%Y-%m-%dT%H:%M:%SZ\') \\\n  --end-time $(date -u \'+%Y-%m-%dT%H:%M:%SZ\')\n\n# Operation 7: Check NGINX controller configuration\nkubectl get configmap -n ingress-nginx\n\n# Operation 8: View NGINX metrics\nkubectl top nodes\n\n# Operation 9: Update ingress with path-based routing\ncat > /tmp/path_based_ingress.yaml <<EOF\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: path-ingress\n  annotations:\n    kubernetes.io/ingress.class: nginx\nspec:\n  rules:\n  - host: multi-app.example.local\n    http:\n      paths:\n      - path: /blue\n        pathType: Prefix\n        backend:\n          service:\n            name: app-blue-service\n            port:\n              number: 80\n      - path: /red\n        pathType: Prefix\n        backend:\n          service:\n            name: app-red-service\n            port:\n              number: 80\nEOF\n\nkubectl apply -f /tmp/path_based_ingress.yaml\n\n# Operation 10: Check service endpoints\nkubectl get endpoints -o wide\n```\n\n---\n\n## Phase 3: Cleanup and Tear-Down\n\n### Cleanup Steps\n```bash\n# Step 1: Delete ingress resources\nkubectl delete ingress --all\n\n# Step 2: Delete deployments and services\nkubectl delete deployment app-blue app-red\nkubectl delete svc app-blue-service app-red-service\n\n# Step 3: Uninstall NGINX Ingress Controller\nhelm uninstall nginx-ingress -n ingress-nginx\n\n# Step 4: Delete namespace\nkubectl delete namespace ingress-nginx\n\n# Step 5: Delete AKS cluster\naz aks delete \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${AKS_CLUSTER}" \\\n  --yes \\\n  --no-wait\n\n# Step 6: Delete the entire resource group\naz group delete \\\n  --name "${RESOURCE_GROUP}" \\\n  --yes \\\n  --no-wait\n\n# Step 7: Verify deletion\nsleep 120\naz group exists --name "${RESOURCE_GROUP}"\n\n# Step 8: Confirm cleanup\necho "Verifying cleanup..."\naz resource list --resource-group "${RESOURCE_GROUP}" 2>&1 | grep "could not be found" && echo "âœ“ Resource group successfully deleted"\n\n# Step 9: Clean up kubeconfig\nkubectl config delete-context "${AKS_CLUSTER}"\nkubectl config delete-cluster "${AKS_CLUSTER}"\n\n# Step 10: Clean up local files\nrm -rf /tmp/Dockerfile_* /tmp/apps_deployment.yaml /tmp/ingress.yaml /tmp/new_ingress.yaml /tmp/path_based_ingress.yaml\n```\n\n---\n\n## Resource Naming Convention\n- Resource Group: `azurehaymaker-containers-ingress-${UNIQUE_ID}-rg`\n- AKS Cluster: `azurehaymaker-aks-ingress-${UNIQUE_ID}`\n- ACR Registry: `azmkringress${UNIQUE_ID}`\n- Virtual Network: `azurehaymaker-vnet-${UNIQUE_ID}`\n- Key Vault: `azurehaymaker-kv-${UNIQUE_ID}`\n\nAll resources tagged with: `AzureHayMaker-managed=true`\n\n---\n\n## Documentation References\n- [NGINX Ingress Controller](https://kubernetes.github.io/ingress-nginx/)\n- [AKS Ingress Controller](https://learn.microsoft.com/en-us/azure/aks/ingress-basic)\n- [Kubernetes Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/)\n- [Helm Package Manager](https://helm.sh/docs/)\n- [AKS Best Practices](https://learn.microsoft.com/en-us/azure/aks/best-practices)\n\n---\n\n## Automation Tool\n**Recommended**: Azure CLI + kubectl + Helm\n\n**Rationale**: Azure CLI provisions AKS, while kubectl and Helm manage ingress controller and applications. This combination is optimal for advanced Kubernetes networking scenarios.\n\n---\n\n## Estimated Duration\n- **Deployment**: 25-30 minutes (AKS + NGINX setup)\n- **Operations Phase**: 8 hours (with routing configuration and monitoring)\n- **Cleanup**: 10-15 minutes\n\n---\n\n## Notes\n- NGINX ingress controller provides advanced routing capabilities\n- Path-based and hostname-based routing supported\n- LoadBalancer service exposes ingress to external traffic\n- Ingress controller can manage SSL/TLS with cert-manager\n- All operations scoped to single tenant and subscription\n- Auto-scaling works at both pod and node levels\n\n\n## Execution Plan\n\n### Phase 1: Test Planning\nPlan test strategy\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: test-design, planning\n\n### Phase 2: Test Implementation\nImplement test cases\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: test-coding, framework-setup\n**Dependencies**: Test Planning\n\n### Phase 3: Test Execution\nRun test suite\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: test-execution, automation\n**Dependencies**: Test Implementation\n\n### Phase 4: Results Analysis\nAnalyze test results\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: analysis, reporting\n**Dependencies**: Test Execution\n\n## Success Criteria\n- Goal \'Scenario: AKS with NGINX Ingress Controller...\' is achieved\n\n## Instructions\nExecute the plan above autonomously:\n1. Follow each phase in sequence\n2. Use available skills and tools\n3. Verify success criteria are met\n4. Report progress and completion',
        "working_dir": ".",
        "sdk": "claude",
        "ui_mode": False,
        "success_criteria": ["Goal 'Scenario: AKS with NGINX Ingress Controller...' is achieved"],
        "constraints": [],
    }

    # Read initial prompt
    prompt_path = Path(__file__).parent / "prompt.md"
    if not prompt_path.exists():
        print("Error: prompt.md not found")
        sys.exit(1)

    initial_prompt = prompt_path.read_text()

    # Create auto-mode instance
    auto_mode = AutoMode(
        sdk=config.get("sdk", "claude"),
        prompt=initial_prompt,
        max_turns=config.get("max_turns", 10),
        working_dir=Path(__file__).parent,
        ui_mode=config.get("ui_mode", False),
    )

    # Run agent
    print("Starting testing-scenario:-aks-nginx-agent...")
    print("Goal: Scenario: AKS with NGINX Ingress Controller")
    print("Estimated duration: 2 hours 36 minutes")
    print()

    exit_code = auto_mode.run()

    if exit_code == 0:
        print("\nGoal achieved successfully!")
    else:
        print(f"\nGoal execution failed with code {exit_code}")

    return exit_code


if __name__ == "__main__":
    sys.exit(main())

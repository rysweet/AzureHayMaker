#!/usr/bin/env python3
"""
testing-scenario:-azure-load-agent - Autonomous Goal-Seeking Agent

Generated by Amplihack Goal Agent Generator
"""

import sys
from pathlib import Path

# Add parent directory to path for amplihack imports
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    from amplihack.launcher.auto_mode import AutoMode
except ImportError:
    print("Error: amplihack package not found")
    print("Install with: pip install amplihack")
    sys.exit(1)


def main():
    """Execute the goal-seeking agent."""
    # Load configuration
    config = {
        "max_turns": 18,
        "initial_prompt": '# Goal: Scenario: Azure Load Balancer with Virtual Machines\n\n## Objective\n# Scenario: Azure Load Balancer with Virtual Machines\n\n## Technology Area\nNetworking\n\n## Company Profile\n- **Company Size**: Mid-size technology company\n- **Industry**: Software / SaaS\n- **Use Case**: Distribute incoming traffic across multiple virtual machines for high availability and fault tolerance\n\n## Scenario Description\nDeploy an Azure Load Balancer configured to distribute HTTP and HTTPS traffic across multiple backend virtual machines. This scenario covers load balancer provisioning, backend pool configuration, health probes, load balancing rules, and inbound NAT rules for management access.\n\n## Azure Services Used\n- Azure Load Balancer (Standard SKU)\n- Azure Virtual Machines\n- Azure Virtual Network\n- Azure Public IP\n- Azure Network Interfaces\n- Azure Network Security Groups\n\n## Prerequisites\n- Azure subscription with Contributor role\n- Azure CLI installed and configured\n- Access to create multiple VMs and load balancers\n\n---\n\n## Phase 1: Deployment and Validation\n\n### Environment Setup\n```bash\n# Set variables\nUNIQUE_ID=$(date +%Y%m%d%H%M%S)\nRESOURCE_GROUP="azurehaymaker-lb-${UNIQUE_ID}-rg"\nLOCATION="eastus"\nVNET_NAME="azurehaymaker-vnet-${UNIQUE_ID}"\nBACKEND_SUBNET="azurehaymaker-backend-${UNIQUE_ID}"\nLB_NAME="azurehaymaker-lb-${UNIQUE_ID}"\nLB_PUBLIC_IP="azurehaymaker-lb-pip-${UNIQUE_ID}"\nBACKEND_POOL="azurehaymaker-backend-pool-${UNIQUE_ID}"\nHEALTH_PROBE="azurehaymaker-health-probe-${UNIQUE_ID}"\nLB_RULE_HTTP="azurehaymaker-http-rule-${UNIQUE_ID}"\nLB_RULE_HTTPS="azurehaymaker-https-rule-${UNIQUE_ID}"\nNSG_NAME="azurehaymaker-lb-nsg-${UNIQUE_ID}"\n\n# Tags\nTAGS="AzureHayMaker-managed=true Scenario=networking-load-balancer Owner=AzureHayMaker"\n```\n\n### Deployment Steps\n```bash\n# Step 1: Create Resource Group\naz group create \\\n  --name "${RESOURCE_GROUP}" \\\n  --location "${LOCATION}" \\\n  --tags ${TAGS}\n\n# Step 2: Create Virtual Network\naz network vnet create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${VNET_NAME}" \\\n  --address-prefix 10.0.0.0/16 \\\n  --tags ${TAGS}\n\n# Step 3: Create Backend Subnet\naz network vnet subnet create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --vnet-name "${VNET_NAME}" \\\n  --name "${BACKEND_SUBNET}" \\\n  --address-prefix 10.0.1.0/24\n\n# Step 4: Create Network Security Group\naz network nsg create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${NSG_NAME}" \\\n  --tags ${TAGS}\n\n# Step 5: Add NSG rules for HTTP, HTTPS, and SSH\naz network nsg rule create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --nsg-name "${NSG_NAME}" \\\n  --name "allow-http" \\\n  --priority 1000 \\\n  --source-address-prefixes "*" \\\n  --source-port-ranges "*" \\\n  --destination-address-prefixes "*" \\\n  --destination-port-ranges 80 \\\n  --access Allow \\\n  --protocol Tcp\n\naz network nsg rule create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --nsg-name "${NSG_NAME}" \\\n  --name "allow-https" \\\n  --priority 1001 \\\n  --source-address-prefixes "*" \\\n  --source-port-ranges "*" \\\n  --destination-address-prefixes "*" \\\n  --destination-port-ranges 443 \\\n  --access Allow \\\n  --protocol Tcp\n\naz network nsg rule create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --nsg-name "${NSG_NAME}" \\\n  --name "allow-ssh" \\\n  --priority 1002 \\\n  --source-address-prefixes "*" \\\n  --source-port-ranges "*" \\\n  --destination-address-prefixes "*" \\\n  --destination-port-ranges 22 \\\n  --access Allow \\\n  --protocol Tcp\n\n# Step 6: Associate NSG with backend subnet\naz network vnet subnet update \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --vnet-name "${VNET_NAME}" \\\n  --name "${BACKEND_SUBNET}" \\\n  --network-security-group "${NSG_NAME}"\n\n# Step 7: Create public IP for Load Balancer\naz network public-ip create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${LB_PUBLIC_IP}" \\\n  --allocation-method Static \\\n  --sku Standard \\\n  --tags ${TAGS}\n\n# Step 8: Create Load Balancer\naz network lb create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${LB_NAME}" \\\n  --sku Standard \\\n  --public-ip-address "${LB_PUBLIC_IP}" \\\n  --frontend-ip-name "azurehaymaker-frontend-${UNIQUE_ID}" \\\n  --backend-pool-name "${BACKEND_POOL}" \\\n  --tags ${TAGS}\n\n# Step 9: Create health probe\naz network lb probe create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --lb-name "${LB_NAME}" \\\n  --name "${HEALTH_PROBE}" \\\n  --protocol http \\\n  --port 80 \\\n  --path "/"\n\n# Step 10: Create load balancing rule for HTTP\naz network lb rule create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --lb-name "${LB_NAME}" \\\n  --name "${LB_RULE_HTTP}" \\\n  --protocol tcp \\\n  --frontend-port 80 \\\n  --backend-port 80 \\\n  --frontend-ip-name "azurehaymaker-frontend-${UNIQUE_ID}" \\\n  --backend-pool-name "${BACKEND_POOL}" \\\n  --probe-name "${HEALTH_PROBE}"\n\n# Step 11: Create load balancing rule for HTTPS\naz network lb rule create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --lb-name "${LB_NAME}" \\\n  --name "${LB_RULE_HTTPS}" \\\n  --protocol tcp \\\n  --frontend-port 443 \\\n  --backend-port 443 \\\n  --frontend-ip-name "azurehaymaker-frontend-${UNIQUE_ID}" \\\n  --backend-pool-name "${BACKEND_POOL}" \\\n  --probe-name "${HEALTH_PROBE}"\n\n# Step 12: Create SSH key pair\nSSH_KEY_NAME="azurehaymaker-key-${UNIQUE_ID}"\naz sshkey create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${SSH_KEY_NAME}" \\\n  --tags ${TAGS}\n\n# Step 13: Create first VM with network interface\nVM1_NAME="azurehaymaker-vm1-${UNIQUE_ID}"\nNIC1_NAME="azurehaymaker-nic1-${UNIQUE_ID}"\n\naz network nic create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${NIC1_NAME}" \\\n  --vnet-name "${VNET_NAME}" \\\n  --subnet "${BACKEND_SUBNET}" \\\n  --lb-name "${LB_NAME}" \\\n  --lb-address-pools "${BACKEND_POOL}" \\\n  --tags ${TAGS}\n\naz vm create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${VM1_NAME}" \\\n  --nics "${NIC1_NAME}" \\\n  --image UbuntuLTS \\\n  --size Standard_B1s \\\n  --admin-username azureuser \\\n  --ssh-key-name "${SSH_KEY_NAME}" \\\n  --os-disk-name "azurehaymaker-osdisk1-${UNIQUE_ID}" \\\n  --tags ${TAGS}\n\n# Step 14: Create second VM with network interface\nVM2_NAME="azurehaymaker-vm2-${UNIQUE_ID}"\nNIC2_NAME="azurehaymaker-nic2-${UNIQUE_ID}"\n\naz network nic create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${NIC2_NAME}" \\\n  --vnet-name "${VNET_NAME}" \\\n  --subnet "${BACKEND_SUBNET}" \\\n  --lb-name "${LB_NAME}" \\\n  --lb-address-pools "${BACKEND_POOL}" \\\n  --tags ${TAGS}\n\naz vm create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${VM2_NAME}" \\\n  --nics "${NIC2_NAME}" \\\n  --image UbuntuLTS \\\n  --size Standard_B1s \\\n  --admin-username azureuser \\\n  --ssh-key-name "${SSH_KEY_NAME}" \\\n  --os-disk-name "azurehaymaker-osdisk2-${UNIQUE_ID}" \\\n  --tags ${TAGS}\n\n# Step 15: Install web server on both VMs\naz vm run-command invoke \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${VM1_NAME}" \\\n  --command-id RunShellScript \\\n  --scripts "sudo apt-get update && sudo apt-get install -y nginx && sudo systemctl start nginx && sudo systemctl enable nginx"\n\naz vm run-command invoke \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${VM2_NAME}" \\\n  --command-id RunShellScript \\\n  --scripts "sudo apt-get update && sudo apt-get install -y nginx && sudo systemctl start nginx && sudo systemctl enable nginx"\n\n# Step 16: Get Load Balancer public IP\nLB_IP=$(az network public-ip show \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${LB_PUBLIC_IP}" \\\n  --query ipAddress -o tsv)\n\necho "Load Balancer Public IP: ${LB_IP}"\n```\n\n### Validation\n```bash\n# Verify Load Balancer\naz network lb show --resource-group "${RESOURCE_GROUP}" --name "${LB_NAME}"\n\n# Verify backend pool\naz network lb address-pool show --resource-group "${RESOURCE_GROUP}" --lb-name "${LB_NAME}" --name "${BACKEND_POOL}"\n\n# Verify health probe\naz network lb probe show --resource-group "${RESOURCE_GROUP}" --lb-name "${LB_NAME}" --name "${HEALTH_PROBE}"\n\n# Verify load balancing rules\naz network lb rule list --resource-group "${RESOURCE_GROUP}" --lb-name "${LB_NAME}" --output table\n\n# Check VM states\naz vm list --resource-group "${RESOURCE_GROUP}" --output table\n\n# Get network interface status\naz network nic show --resource-group "${RESOURCE_GROUP}" --name "${NIC1_NAME}" --query "ipConfigurations"\n\n# Test HTTP connectivity\necho "Testing Load Balancer connectivity..."\ncurl -I "http://${LB_IP}/" || echo "Note: May need to wait for VMs to fully initialize"\n\n# List all resources\naz resource list --resource-group "${RESOURCE_GROUP}" --output table\n```\n\n---\n\n## Phase 2: Mid-Day Operations and Management\n\n### Management Operations\n```bash\n# Operation 1: Check backend pool health status\naz network lb address-pool list \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --lb-name "${LB_NAME}" \\\n  --output table\n\n# Operation 2: Monitor load balancer metrics - throughput\naz monitor metrics list \\\n  --resource "/subscriptions/$(az account show --query id -o tsv)/resourceGroups/${RESOURCE_GROUP}/providers/Microsoft.Network/loadBalancers/${LB_NAME}" \\\n  --metric "BytesIn BytesOut" \\\n  --start-time $(date -u -d \'1 hour ago\' \'+%Y-%m-%dT%H:%M:%SZ\') \\\n  --end-time $(date -u \'+%Y-%m-%dT%H:%M:%SZ\')\n\n# Operation 3: Monitor load balancer metrics - connections\naz monitor metrics list \\\n  --resource "/subscriptions/$(az account show --query id -o tsv)/resourceGroups/${RESOURCE_GROUP}/providers/Microsoft.Network/loadBalancers/${LB_NAME}" \\\n  --metric "SNAT Connection Count" \\\n  --start-time $(date -u -d \'1 hour ago\' \'+%Y-%m-%dT%H:%M:%SZ\') \\\n  --end-time $(date -u \'+%Y-%m-%dT%H:%M:%SZ\')\n\n# Operation 4: Restart health probe to verify detection\naz network lb probe update \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --lb-name "${LB_NAME}" \\\n  --name "${HEALTH_PROBE}" \\\n  --protocol http \\\n  --port 80 \\\n  --path "/"\n\n# Operation 5: Create inbound NAT rule for SSH to VM1\nNAT_RULE1="azurehaymaker-nat-vm1-${UNIQUE_ID}"\naz network lb inbound-nat-rule create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --lb-name "${LB_NAME}" \\\n  --name "${NAT_RULE1}" \\\n  --protocol Tcp \\\n  --frontend-port 2201 \\\n  --backend-port 22 \\\n  --frontend-ip-name "azurehaymaker-frontend-${UNIQUE_ID}"\n\n# Operation 6: Create inbound NAT rule for SSH to VM2\nNAT_RULE2="azurehaymaker-nat-vm2-${UNIQUE_ID}"\naz network lb inbound-nat-rule create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --lb-name "${LB_NAME}" \\\n  --name "${NAT_RULE2}" \\\n  --protocol Tcp \\\n  --frontend-port 2202 \\\n  --backend-port 22 \\\n  --frontend-ip-name "azurehaymaker-frontend-${UNIQUE_ID}"\n\n# Operation 7: Associate NAT rules with NICs\naz network nic ip-config inbound-nat-rule add \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --nic-name "${NIC1_NAME}" \\\n  --ip-config-name "ipconfig${VM1_NAME}" \\\n  --inbound-nat-rule "${NAT_RULE1}" \\\n  --lb-name "${LB_NAME}"\n\naz network nic ip-config inbound-nat-rule add \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --nic-name "${NIC2_NAME}" \\\n  --ip-config-name "ipconfig${VM2_NAME}" \\\n  --inbound-nat-rule "${NAT_RULE2}" \\\n  --lb-name "${LB_NAME}"\n\n# Operation 8: Update VM1 with custom web content\naz vm run-command invoke \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${VM1_NAME}" \\\n  --command-id RunShellScript \\\n  --scripts "sudo tee /var/www/html/index.html > /dev/null <<\'EOF\'\n<!DOCTYPE html>\n<html>\n<head><title>Server 1</title></head>\n<body><h1>Azure Load Balancer - Server 1</h1><p>Hostname: $(hostname)</p></body>\n</html>\nEOF"\n\n# Operation 9: Update VM2 with custom web content\naz vm run-command invoke \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${VM2_NAME}" \\\n  --command-id RunShellScript \\\n  --scripts "sudo tee /var/www/html/index.html > /dev/null <<\'EOF\'\n<!DOCTYPE html>\n<html>\n<head><title>Server 2</title></head>\n<body><h1>Azure Load Balancer - Server 2</h1><p>Hostname: $(hostname)</p></body>\n</html>\nEOF"\n\n# Operation 10: Check outbound rules\naz network lb outbound-rule list \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --lb-name "${LB_NAME}" \\\n  --output table\n\n# Operation 11: Monitor individual VM metrics\naz monitor metrics list \\\n  --resource "/subscriptions/$(az account show --query id -o tsv)/resourceGroups/${RESOURCE_GROUP}/providers/Microsoft.Compute/virtualMachines/${VM1_NAME}" \\\n  --metric "Percentage CPU" \\\n  --start-time $(date -u -d \'30 minutes ago\' \'+%Y-%m-%dT%H:%M:%SZ\') \\\n  --end-time $(date -u \'+%Y-%m-%dT%H:%M:%SZ\')\n\n# Operation 12: Verify network interface status and IP configuration\naz network nic show \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${NIC1_NAME}" \\\n  --query "{name: name, ipConfigurations: ipConfigurations, privateIpAddress: ipConfigurations[0].privateIpAddress}"\n```\n\n---\n\n## Phase 3: Cleanup and Tear-Down\n\n### Cleanup Steps\n```bash\n# Step 1: Delete the entire resource group (includes load balancer, VMs, NICs, storage, etc.)\naz group delete \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --yes \\\n  --no-wait\n\n# Step 2: Wait for deletion to complete\necho "Waiting for resource group deletion..."\nsleep 120\n\n# Step 3: Verify deletion\naz group exists --name "${RESOURCE_GROUP}"\n\n# Step 4: Confirm cleanup\necho "Verifying cleanup..."\naz resource list --resource-group "${RESOURCE_GROUP}" 2>&1 | grep "could not be found" && echo "âœ“ Resource group successfully deleted"\n```\n\n---\n\n## Resource Naming Convention\n- Resource Group: `azurehaymaker-lb-${UNIQUE_ID}-rg`\n- Virtual Network: `azurehaymaker-vnet-${UNIQUE_ID}`\n- Load Balancer: `azurehaymaker-lb-${UNIQUE_ID}`\n- Load Balancer Public IP: `azurehaymaker-lb-pip-${UNIQUE_ID}`\n- Backend Pool: `azurehaymaker-backend-pool-${UNIQUE_ID}`\n- Health Probe: `azurehaymaker-health-probe-${UNIQUE_ID}`\n- Virtual Machines: `azurehaymaker-vm1-${UNIQUE_ID}`, `azurehaymaker-vm2-${UNIQUE_ID}`\n\nAll resources tagged with: `AzureHayMaker-managed=true`\n\n---\n\n## Documentation References\n- [Azure Load Balancer Overview](https://learn.microsoft.com/en-us/azure/load-balancer/load-balancer-overview)\n- [Create Azure Load Balancer with CLI](https://learn.microsoft.com/en-us/azure/load-balancer/quickstart-load-balancer-standard-public-cli)\n- [Health Probes](https://learn.microsoft.com/en-us/azure/load-balancer/load-balancer-custom-probe-overview)\n- [Inbound NAT Rules](https://learn.microsoft.com/en-us/azure/load-balancer/load-balancer-inbound-nat-rules)\n- [Load Balancer CLI Reference](https://learn.microsoft.com/en-us/cli/azure/network/lb)\n\n---\n\n## Automation Tool\n**Recommended**: Azure CLI\n\n**Rationale**: Azure CLI provides comprehensive load balancer management with straightforward commands for provisioning, configuration of rules and health probes, and backend pool management. Direct CLI commands are ideal for managing distributed application workloads.\n\n---\n\n## Estimated Duration\n- **Deployment**: 15-20 minutes (includes VM creation and software installation)\n- **Operations Phase**: 8+ hours (with monitoring, updates, and load distribution verification)\n- **Cleanup**: 10-15 minutes\n\n---\n\n## Notes\n- Load Balancer uses Standard SKU for production-ready functionality\n- Health probe monitors HTTP port 80 on backend VMs\n- Two VMs provide redundancy for demonstration purposes\n- NAT rules enable direct SSH access to individual VMs through load balancer\n- Custom web content differentiates responses from each VM\n- All operations scoped to single tenant and subscription\n- SSH keys securely managed by Azure\n- Backend pool automatically distributes traffic across healthy VMs\n\n\n## Execution Plan\n\n### Phase 1: Test Planning\nPlan test strategy\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: test-design, planning\n\n### Phase 2: Test Implementation\nImplement test cases\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: test-coding, framework-setup\n**Dependencies**: Test Planning\n\n### Phase 3: Test Execution\nRun test suite\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: test-execution, automation\n**Dependencies**: Test Implementation\n\n### Phase 4: Results Analysis\nAnalyze test results\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: analysis, reporting\n**Dependencies**: Test Execution\n\n## Success Criteria\n- Goal \'Scenario: Azure Load Balancer with Virtual Machine...\' is achieved\n\n## Instructions\nExecute the plan above autonomously:\n1. Follow each phase in sequence\n2. Use available skills and tools\n3. Verify success criteria are met\n4. Report progress and completion',
        "working_dir": ".",
        "sdk": "claude",
        "ui_mode": False,
        "success_criteria": [
            "Goal 'Scenario: Azure Load Balancer with Virtual Machine...' is achieved"
        ],
        "constraints": [],
    }

    # Read initial prompt
    prompt_path = Path(__file__).parent / "prompt.md"
    if not prompt_path.exists():
        print("Error: prompt.md not found")
        sys.exit(1)

    initial_prompt = prompt_path.read_text()

    # Create auto-mode instance
    auto_mode = AutoMode(
        sdk=config.get("sdk", "claude"),
        prompt=initial_prompt,
        max_turns=config.get("max_turns", 10),
        working_dir=Path(__file__).parent,
        ui_mode=config.get("ui_mode", False),
    )

    # Run agent
    print("Starting testing-scenario:-azure-load-agent...")
    print("Goal: Scenario: Azure Load Balancer with Virtual Machines")
    print("Estimated duration: 2 hours 36 minutes")
    print()

    exit_code = auto_mode.run()

    if exit_code == 0:
        print("\nGoal achieved successfully!")
    else:
        print(f"\nGoal execution failed with code {exit_code}")

    return exit_code


if __name__ == "__main__":
    sys.exit(main())

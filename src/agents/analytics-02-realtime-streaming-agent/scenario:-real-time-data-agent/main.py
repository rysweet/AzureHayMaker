#!/usr/bin/env python3
"""
scenario:-real-time-data-agent - Autonomous Goal-Seeking Agent

Generated by Amplihack Goal Agent Generator
"""

import sys
from pathlib import Path

# Add parent directory to path for amplihack imports
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    from amplihack.launcher.auto_mode import AutoMode
except ImportError:
    print("Error: amplihack package not found")
    print("Install with: pip install amplihack")
    sys.exit(1)


def main():
    """Execute the goal-seeking agent."""
    # Load configuration
    config = {
        "max_turns": 18,
        "initial_prompt": '# Goal: Scenario: Real-Time Data Streaming with Azure Stream Analytics\n\n## Objective\n# Scenario: Real-Time Data Streaming with Azure Stream Analytics\n\n## Technology Area\nAnalytics\n\n## Company Profile\n- **Company Size**: Mid-size financial services company\n- **Industry**: Financial Services / Trading\n- **Use Case**: Ingest and analyze stock market data in real-time, detect anomalies, and trigger alerts\n\n## Scenario Description\nDeploy Azure Stream Analytics to process real-time data streams from Event Hubs, perform windowed aggregations, detect trading anomalies, and output results to Azure SQL Database and Power BI for live dashboarding.\n\n## Azure Services Used\n- Azure Event Hubs (data ingestion)\n- Azure Stream Analytics (real-time processing)\n- Azure SQL Database (results storage)\n- Azure Storage (checkpoint storage)\n- Azure Key Vault (connection strings)\n\n## Prerequisites\n- Azure subscription with Contributor role\n- Azure CLI installed\n- A unique identifier for this scenario run\n\n---\n\n## Phase 1: Deployment and Validation\n\n### Environment Setup\n```bash\n# Set variables\nUNIQUE_ID=$(date +%Y%m%d%H%M%S)\nRESOURCE_GROUP="azurehaymaker-analytics-streaming-${UNIQUE_ID}-rg"\nLOCATION="eastus"\nEVENT_HUB_NS="azurehaymaker-eh-${UNIQUE_ID}"\nEVENT_HUB_NAME="stock-data-stream"\nSTREAM_ANALYTICS_JOB="azurehaymaker-asa-${UNIQUE_ID}"\nSTORAGE_ACCOUNT="azmkrstream${UNIQUE_ID}"\nSQL_SERVER="azurehaymaker-sql-${UNIQUE_ID}"\nSQL_DB="analyticsdb"\nSQL_ADMIN_USER="sqladmin"\nSQL_ADMIN_PASSWORD="P@ssw0rd!${UNIQUE_ID}"\n\n# Tags\nTAGS="AzureHayMaker-managed=true Scenario=analytics-realtime-streaming Owner=AzureHayMaker"\n```\n\n### Deployment Steps\n```bash\n# Step 1: Create Resource Group\naz group create \\\n  --name "${RESOURCE_GROUP}" \\\n  --location "${LOCATION}" \\\n  --tags ${TAGS}\n\n# Step 2: Create Storage Account for checkpoint storage\naz storage account create \\\n  --name "${STORAGE_ACCOUNT}" \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --location "${LOCATION}" \\\n  --sku Standard_LRS \\\n  --kind StorageV2 \\\n  --tags ${TAGS}\n\nSTORAGE_KEY=$(az storage account keys list \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --account-name "${STORAGE_ACCOUNT}" \\\n  --query \'[0].value\' -o tsv)\n\n# Step 3: Create Event Hubs namespace\naz eventhubs namespace create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${EVENT_HUB_NS}" \\\n  --location "${LOCATION}" \\\n  --sku Standard \\\n  --tags ${TAGS}\n\n# Step 4: Create Event Hub\naz eventhubs eventhub create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --namespace-name "${EVENT_HUB_NS}" \\\n  --name "${EVENT_HUB_NAME}" \\\n  --message-retention 1 \\\n  --partition-count 4\n\n# Step 5: Create Event Hub authorization rule\naz eventhubs eventhub authorization-rule create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --namespace-name "${EVENT_HUB_NS}" \\\n  --eventhub-name "${EVENT_HUB_NAME}" \\\n  --name "ListenSendRule" \\\n  --rights Send Listen\n\n# Step 6: Get Event Hub connection string\nEVENT_HUB_CONNECTION_STRING=$(az eventhubs eventhub authorization-rule keys list \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --namespace-name "${EVENT_HUB_NS}" \\\n  --eventhub-name "${EVENT_HUB_NAME}" \\\n  --name "ListenSendRule" \\\n  --query primaryConnectionString -o tsv)\n\n# Step 7: Create SQL Server and Database\naz sql server create \\\n  --name "${SQL_SERVER}" \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --location "${LOCATION}" \\\n  --admin-user "${SQL_ADMIN_USER}" \\\n  --admin-password "${SQL_ADMIN_PASSWORD}" \\\n  --tags ${TAGS}\n\naz sql db create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --server "${SQL_SERVER}" \\\n  --name "${SQL_DB}" \\\n  --service-objective S0 \\\n  --tags ${TAGS}\n\n# Step 8: Configure SQL firewall\naz sql server firewall-rule create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --server "${SQL_SERVER}" \\\n  --name "AllowAzureServices" \\\n  --start-ip-address 0.0.0.0 \\\n  --end-ip-address 0.0.0.0\n\n# Step 9: Create target table in SQL Database\nsqlcmd -S "${SQL_SERVER}.database.windows.net" -U "${SQL_ADMIN_USER}" -P "${SQL_ADMIN_PASSWORD}" -d "${SQL_DB}" -Q "\nCREATE TABLE StockAnomalies (\n    EventId NVARCHAR(50),\n    StockSymbol NVARCHAR(10),\n    Price DECIMAL(10,2),\n    AnomalyScore DECIMAL(5,2),\n    WindowTime DATETIME,\n    EventTime DATETIME\n);"\n\n# Step 10: Create Stream Analytics Job\naz stream-analytics job create \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --job-name "${STREAM_ANALYTICS_JOB}" \\\n  --location "${LOCATION}" \\\n  --output-start-mode LastOutputEventTime \\\n  --events-outoforder-policy Adjust \\\n  --events-outoforder-max-delay 10 \\\n  --tags ${TAGS}\n\necho "Stream Analytics job created: ${STREAM_ANALYTICS_JOB}"\n```\n\n### Validation\n```bash\n# Verify Resource Group\naz group show --name "${RESOURCE_GROUP}"\n\n# Verify Event Hubs namespace\naz eventhubs namespace show \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --name "${EVENT_HUB_NS}"\n\n# Verify Event Hub\naz eventhubs eventhub show \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --namespace-name "${EVENT_HUB_NS}" \\\n  --name "${EVENT_HUB_NAME}"\n\n# Verify SQL Database\naz sql db show \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --server "${SQL_SERVER}" \\\n  --name "${SQL_DB}"\n\n# Verify Stream Analytics Job\naz stream-analytics job show \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --job-name "${STREAM_ANALYTICS_JOB}"\n\n# List all resources\naz resource list --resource-group "${RESOURCE_GROUP}" --output table\n```\n\n---\n\n## Phase 2: Mid-Day Operations and Management\n\n### Management Operations\n```bash\n# Operation 1: Simulate sending events to Event Hub\nfor i in {1..10}; do\n  SYMBOL=("AAPL" "MSFT" "GOOG" "AMZN" "TSLA")\n  STOCK=${SYMBOL[$((RANDOM % 5))]}\n  PRICE=$((100 + RANDOM % 100))\n\n  echo "{\\"stockSymbol\\":\\"${STOCK}\\",\\"price\\":${PRICE},\\"timestamp\\":\\"$(date -u \'+%Y-%m-%dT%H:%M:%SZ\')\\"}" | \\\n  az eventhubs eventhub send \\\n    --resource-group "${RESOURCE_GROUP}" \\\n    --namespace-name "${EVENT_HUB_NS}" \\\n    --name "${EVENT_HUB_NAME}" \\\n    --connection-string "${EVENT_HUB_CONNECTION_STRING}"\n\n  sleep 1\ndone\n\n# Operation 2: Start Stream Analytics Job\naz stream-analytics job start \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --job-name "${STREAM_ANALYTICS_JOB}" \\\n  --output-start-mode LastOutputEventTime\n\n# Operation 3: Check job status\naz stream-analytics job show \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --job-name "${STREAM_ANALYTICS_JOB}" \\\n  --query "jobState" -o tsv\n\n# Operation 4: Monitor Event Hub metrics\naz monitor metrics list \\\n  --resource "/subscriptions/$(az account show --query id -o tsv)/resourceGroups/${RESOURCE_GROUP}/providers/Microsoft.EventHub/namespaces/${EVENT_HUB_NS}" \\\n  --metric "IncomingMessages" \\\n  --start-time $(date -u -d \'1 hour ago\' \'+%Y-%m-%dT%H:%M:%SZ\') \\\n  --end-time $(date -u \'+%Y-%m-%dT%H:%M:%SZ\')\n\n# Operation 5: Query anomalies from SQL\nsqlcmd -S "${SQL_SERVER}.database.windows.net" -U "${SQL_ADMIN_USER}" -P "${SQL_ADMIN_PASSWORD}" -d "${SQL_DB}" -Q "SELECT COUNT(*) as AnomalyCount FROM StockAnomalies;"\n\n# Operation 6: Stop Stream Analytics Job\naz stream-analytics job stop \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --job-name "${STREAM_ANALYTICS_JOB}"\n\n# Operation 7: Monitor Stream Analytics metrics\naz monitor metrics list \\\n  --resource "/subscriptions/$(az account show --query id -o tsv)/resourceGroups/${RESOURCE_GROUP}/providers/Microsoft.StreamAnalytics/streamingjobs/${STREAM_ANALYTICS_JOB}" \\\n  --metric "CPU%" \\\n  --start-time $(date -u -d \'1 hour ago\' \'+%Y-%m-%dT%H:%M:%SZ\') \\\n  --end-time $(date -u \'+%Y-%m-%dT%H:%M:%SZ\')\n\n# Operation 8: List Event Hub partitions\naz eventhubs eventhub show \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --namespace-name "${EVENT_HUB_NS}" \\\n  --name "${EVENT_HUB_NAME}" \\\n  --query "partitionCount" -o tsv\n```\n\n---\n\n## Phase 3: Cleanup and Tear-Down\n\n### Cleanup Steps\n```bash\n# Step 1: Stop Stream Analytics Job if running\naz stream-analytics job stop \\\n  --resource-group "${RESOURCE_GROUP}" \\\n  --job-name "${STREAM_ANALYTICS_JOB}" \\\n  --no-wait\n\n# Step 2: Delete the entire resource group\naz group delete \\\n  --name "${RESOURCE_GROUP}" \\\n  --yes \\\n  --no-wait\n\n# Step 3: Verify deletion\nsleep 120\naz group exists --name "${RESOURCE_GROUP}"\n\n# Step 4: Confirm cleanup\necho "Verifying cleanup..."\naz resource list --resource-group "${RESOURCE_GROUP}" 2>&1 | grep "could not be found" && echo "âœ“ Resource group successfully deleted"\n```\n\n---\n\n## Resource Naming Convention\n- Resource Group: `azurehaymaker-analytics-streaming-${UNIQUE_ID}-rg`\n- Event Hubs Namespace: `azurehaymaker-eh-${UNIQUE_ID}`\n- Event Hub: `stock-data-stream`\n- Stream Analytics Job: `azurehaymaker-asa-${UNIQUE_ID}`\n- Storage Account: `azmkrstream${UNIQUE_ID}`\n- SQL Server: `azurehaymaker-sql-${UNIQUE_ID}`\n\nAll resources tagged with: `AzureHayMaker-managed=true`\n\n---\n\n## Documentation References\n- [Azure Stream Analytics Overview](https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-introduction)\n- [Stream Analytics Query Language](https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-query-language)\n- [Azure Event Hubs Overview](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-about)\n- [Stream Analytics CLI Reference](https://learn.microsoft.com/en-us/cli/azure/stream-analytics)\n- [Stream Analytics Windowing Functions](https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions)\n\n---\n\n## Automation Tool\n**Recommended**: Azure CLI\n\n**Rationale**: Azure CLI provides excellent support for Stream Analytics, Event Hubs, and SQL services. While the query definition requires JSON, Azure CLI handles the orchestration efficiently for this scenario.\n\n---\n\n## Estimated Duration\n- **Deployment**: 15-20 minutes\n- **Operations Phase**: 8 hours (with continuous streaming and monitoring)\n- **Cleanup**: 5-10 minutes\n\n---\n\n## Notes\n- Stream Analytics can process millions of events per second\n- Checkpointing to storage ensures no data loss on failures\n- Window functions enable real-time aggregations and anomaly detection\n- All operations scoped to single tenant and subscription\n- Connection strings securely stored for production use\n- Adjust event out-of-order policy based on data characteristics\n\n\n## Execution Plan\n\n### Phase 1: Data Collection\nGather required data from sources\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: data-ingestion, validation\n\n### Phase 2: Data Transformation\nTransform and clean data\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: parsing, transformation\n**Dependencies**: Data Collection\n\n### Phase 3: Data Analysis\nAnalyze processed data\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: analysis, pattern-detection\n**Dependencies**: Data Transformation\n\n### Phase 4: Report Generation\nGenerate results and reports\n**Estimated Duration**: 30 minutes\n**Required Capabilities**: reporting, visualization\n**Dependencies**: Data Analysis\n\n## Success Criteria\n- Goal \'Scenario: Real-Time Data Streaming with Azure Stre...\' is achieved\n\n## Instructions\nExecute the plan above autonomously:\n1. Follow each phase in sequence\n2. Use available skills and tools\n3. Verify success criteria are met\n4. Report progress and completion',
        "working_dir": ".",
        "sdk": "claude",
        "ui_mode": False,
        "success_criteria": [
            "Goal 'Scenario: Real-Time Data Streaming with Azure Stre...' is achieved"
        ],
        "constraints": [],
    }

    # Read initial prompt
    prompt_path = Path(__file__).parent / "prompt.md"
    if not prompt_path.exists():
        print("Error: prompt.md not found")
        sys.exit(1)

    initial_prompt = prompt_path.read_text()

    # Create auto-mode instance
    auto_mode = AutoMode(
        sdk=config.get("sdk", "claude"),
        prompt=initial_prompt,
        max_turns=config.get("max_turns", 10),
        working_dir=Path(__file__).parent,
        ui_mode=config.get("ui_mode", False),
    )

    # Run agent
    print("Starting scenario:-real-time-data-agent...")
    print("Goal: Scenario: Real-Time Data Streaming with Azure Stream Analytics")
    print("Estimated duration: 2 hours 36 minutes")
    print()

    exit_code = auto_mode.run()

    if exit_code == 0:
        print("\nGoal achieved successfully!")
    else:
        print(f"\nGoal execution failed with code {exit_code}")

    return exit_code


if __name__ == "__main__":
    sys.exit(main())
